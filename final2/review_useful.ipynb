{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "# 讀取 CSV 檔案\n",
    "df = pl.read_csv(\"amazon_2023.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整理評論相關欄位:\n",
    "- title_length: 標題長度\n",
    "- text_length: 文本長度\n",
    "- user_avg_text_length: 該用戶所有評論的平均文本長度\n",
    "- user_review_count: 該用戶的評論總數\n",
    "- days_since_review_ln : 評論發布距今天數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 處理評論數據並計算基本屬性\n",
    "processed_df = df.with_columns([\n",
    "    # 1. 計算 title 長度\n",
    "    pl.col(\"title\").str.len_chars().alias(\"title_length\"),\n",
    "    \n",
    "    # 2. 計算 text 長度\n",
    "    pl.col(\"text\").str.len_chars().alias(\"text_length\"),\n",
    "    \n",
    "    # 5. 計算評論發布距今天數（自然對數）\n",
    "    # unix timestamp 轉換為天數差異再取對數\n",
    "    (pl.lit(1750089600000) - (pl.col(\"timestamp\")) / 86400000).log().alias(\"days_since_review_ln\")\n",
    "]).with_columns([\n",
    "    # 3. 計算每個 user_id 的平均 text 長度\n",
    "    pl.col(\"text_length\").mean().over(\"user_id\").alias(\"user_avg_text_length\"),\n",
    "    \n",
    "    # 4. 計算每個 user_id 的評論總數\n",
    "    pl.col(\"user_id\").count().over(\"user_id\").alias(\"user_review_count\")\n",
    "]).with_columns([\n",
    "    # 取自然對數處理\n",
    "    pl.col(\"title_length\").log().alias(\"title_length_ln\"),\n",
    "    pl.col(\"text_length\").log().alias(\"text_length_ln\"),\n",
    "    pl.col(\"user_avg_text_length\").log().alias(\"user_avg_text_length_ln\"),\n",
    "    pl.col(\"user_review_count\").log().alias(\"user_review_count_ln\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 32)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>rating</th><th>title</th><th>text</th><th>images</th><th>asin</th><th>parent_asin</th><th>user_id</th><th>timestamp</th><th>helpful_vote</th><th>verified_purchase</th><th>main_category</th><th>average_rating</th><th>rating_number</th><th>features</th><th>description</th><th>price</th><th>videos</th><th>store</th><th>categories</th><th>details</th><th>bought_together</th><th>subtitle</th><th>author</th><th>title_length</th><th>text_length</th><th>days_since_review_ln</th><th>user_avg_text_length</th><th>user_review_count</th><th>title_length_ln</th><th>text_length_ln</th><th>user_avg_text_length_ln</th><th>user_review_count_ln</th></tr><tr><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>bool</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>u32</td><td>u32</td><td>f64</td><td>f64</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.0</td><td>&quot;Watercolor with Me in the Ocea…</td><td>&quot;It is definitely not a waterco…</td><td>&quot;{&#x27;hi_res&#x27;: [None], &#x27;large&#x27;: [&#x27;…</td><td>&quot;B09BGPFTDB&quot;</td><td>&quot;B09BGPFTDB&quot;</td><td>&quot;AFKZENTNBQ7A7V7UXW5JJI6UGRYQ&quot;</td><td>1642399598485</td><td>0</td><td>true</td><td>&quot;Books&quot;</td><td>1.0</td><td>1.0</td><td>&quot;[&#x27;This incredible adult colori…</td><td>&quot;[]&quot;</td><td>&quot;5.99&quot;</td><td>&quot;{&#x27;title&#x27;: [], &#x27;url&#x27;: [], &#x27;user…</td><td>&quot;LEONARD DRAVEN (Author)&quot;</td><td>&quot;[&#x27;Books&#x27;, &#x27;Arts &amp; Photography&#x27;…</td><td>&quot;{&quot;Publisher&quot;: &quot;Independently p…</td><td>null</td><td>&quot;Paperback – July 29, 2021&quot;</td><td>null</td><td>56</td><td>1427</td><td>28.190688</td><td>731.173913</td><td>23</td><td>4.025352</td><td>7.26333</td><td>6.594651</td><td>3.135494</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 32)\n",
       "┌────────┬────────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ rating ┆ title      ┆ text       ┆ images    ┆ … ┆ title_len ┆ text_leng ┆ user_avg_ ┆ user_revi │\n",
       "│ ---    ┆ ---        ┆ ---        ┆ ---       ┆   ┆ gth_ln    ┆ th_ln     ┆ text_leng ┆ ew_count_ │\n",
       "│ f64    ┆ str        ┆ str        ┆ str       ┆   ┆ ---       ┆ ---       ┆ th_ln     ┆ ln        │\n",
       "│        ┆            ┆            ┆           ┆   ┆ f64       ┆ f64       ┆ ---       ┆ ---       │\n",
       "│        ┆            ┆            ┆           ┆   ┆           ┆           ┆ f64       ┆ f64       │\n",
       "╞════════╪════════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 1.0    ┆ Watercolor ┆ It is      ┆ {'hi_res' ┆ … ┆ 4.025352  ┆ 7.26333   ┆ 6.594651  ┆ 3.135494  │\n",
       "│        ┆ with Me in ┆ definitely ┆ : [None], ┆   ┆           ┆           ┆           ┆           │\n",
       "│        ┆ the Ocea…  ┆ not a      ┆ 'large':  ┆   ┆           ┆           ┆           ┆           │\n",
       "│        ┆            ┆ waterco…   ┆ ['…       ┆   ┆           ┆           ┆           ┆           │\n",
       "└────────┴────────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 標準化並淨化文本數據\n",
    "1. 將所有單詞轉換為小寫 (transforming all words into lowercase)\n",
    "2. 刪除所有停用詞、標點符號、數位和空格 (removing all stop words, punctuation marks, numbers, and spaces)\n",
    "3. 對單詞進行詞幹提取（例如，values、valued 和 valuing 都替換為 value）(stemming words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChenChun\\.conda\\envs\\myenv\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\ChenChun\\AppData\\Local\\Temp\\ipykernel_26844\\3043388951.py:39: DtypeWarning: Columns (15,21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"amazon_2023.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本預處理完成！\n",
      "處理了 409482 筆資料\n",
      "\n",
      "預處理前後對比範例:\n",
      "==================================================\n",
      "原始title: Watercolor with Me in the Ocean: Coloring book for Adult\n",
      "處理後title: watercolor ocean color book adult\n",
      "\n",
      "原始text: It is definitely not a watercolor book.  The paper bucked completely.  The pages honestly appear to ...\n",
      "處理後text: definit watercolor book paper buck complet page honestli appear photo copi pictur say bc look seal p...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# 下載必要的NLTK資源\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    文本預處理函數\n",
    "    1. 轉換為小寫\n",
    "    2. 刪除停用詞、標點符號、數字和多餘空格\n",
    "    3. 詞幹提取\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # 步驟1: 轉換為小寫\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 步驟2: 刪除標點符號和數字，只保留字母和空格\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 分割單詞並刪除停用詞\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word and word not in stop_words]\n",
    "    \n",
    "    # 步驟3: 詞幹提取\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # 移除空字符串並重新組合\n",
    "    return ' '.join([word for word in words if word])\n",
    "\n",
    "# 讀取CSV文件\n",
    "df = pd.read_csv(\"amazon_2023.csv\")\n",
    "\n",
    "# 對title和text欄位進行預處理\n",
    "df['title_processed'] = df['title'].apply(preprocess_text)\n",
    "df['text_processed'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 保存處理後的結果\n",
    "df.to_csv(\"amazon_2023_processed.csv\", index=False)\n",
    "\n",
    "print(\"文本預處理完成！\")\n",
    "print(f\"處理了 {len(df)} 筆資料\")\n",
    "print(\"\\n預處理前後對比範例:\")\n",
    "print(\"=\" * 50)\n",
    "if len(df) > 0:\n",
    "    print(\"原始title:\", df['title'].iloc[0][:100] + \"...\" if len(str(df['title'].iloc[0])) > 100 else df['title'].iloc[0])\n",
    "    print(\"處理後title:\", df['title_processed'].iloc[0][:100] + \"...\" if len(str(df['title_processed'].iloc[0])) > 100 else df['title_processed'].iloc[0])\n",
    "    print(\"\\n原始text:\", df['text'].iloc[0][:100] + \"...\" if len(str(df['text'].iloc[0])) > 100 else df['text'].iloc[0])\n",
    "    print(\"處理後text:\", df['text_processed'].iloc[0][:100] + \"...\" if len(str(df['text_processed'].iloc[0])) > 100 else df['text_processed'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotional Feature Extraction\n",
    "在文本預處理之後，研究使用特定的字典來量化每條評論中的情感效價 (valence) 和喚醒度 (arousal)。<br>\n",
    "使用預先量化的情感字典：該研究使用了 Warriner 等人 開發的字典，該字典量化了近 14,000 個英文單詞的情感效價和喚醒度分數。這種方法超越了僅僅計算正面或負面詞彙的數量，能夠更好地捕捉情感的強度。\n",
    "\n",
    "1. 建立 Valence 欄位\n",
    "A review's overall valence measured by the difference between scores of positive words and negative words\n",
    "<br>評價的總體效價由正面詞和負面詞的分數之間的差異來衡量\n",
    "\n",
    "2. 建立 Arousal 欄位\n",
    "The level of emotion activation measured by scores of arousal words in a review.\n",
    "<br>通過評論中喚醒詞的分數來衡量的情緒激活水準。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入數據\n",
    "df = pd.read_csv(\"amazon_2023_processed.csv\")\n",
    "emotion_dict = pd.read_csv(\"springer_data/BRM-emot-submit.csv\")\n",
    "\n",
    "# 建立詞彙-情感分數映射\n",
    "word_scores = {}\n",
    "for _, row in emotion_dict.iterrows():\n",
    "    word_scores[str(row['Word']).lower()] = {\n",
    "        'valence': row['V.Mean.Sum'],\n",
    "        'arousal': row['A.Mean.Sum']\n",
    "    }\n",
    "\n",
    "# 計算情感分數函數\n",
    "def calculate_emotion_scores(text):\n",
    "    if pd.isna(text):\n",
    "        return 0, 0\n",
    "    \n",
    "    words = str(text).lower().split()\n",
    "    \n",
    "    # 計算效價和喚醒度\n",
    "    positive_scores = []\n",
    "    negative_scores = []\n",
    "    arousal_scores = []\n",
    "    \n",
    "    for word in words:\n",
    "        word = word.strip('.,!?\";:()[]{}')\n",
    "        if word in word_scores:\n",
    "            valence = word_scores[word]['valence']\n",
    "            arousal = word_scores[word]['arousal']\n",
    "            \n",
    "            if valence > 5:\n",
    "                positive_scores.append(valence)\n",
    "            elif valence < 5:\n",
    "                negative_scores.append(valence)\n",
    "            \n",
    "            arousal_scores.append(arousal)\n",
    "    \n",
    "    # 效價 = 正面分數總和 - 負面分數總和\n",
    "    valence_score = sum(positive_scores) - sum(negative_scores)\n",
    "    # 喚醒度 = 所有喚醒度分數總和\n",
    "    arousal_score = sum(arousal_scores)\n",
    "    \n",
    "    return valence_score, arousal_score\n",
    "\n",
    "# 計算每條評論的情感分數\n",
    "valence_scores = []\n",
    "arousal_scores = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # 合併標題和文本\n",
    "    combined_text = str(row['title_processed']) + \" \" + str(row['text_processed'])\n",
    "    valence, arousal = calculate_emotion_scores(combined_text)\n",
    "    valence_scores.append(valence)\n",
    "    arousal_scores.append(arousal)\n",
    "\n",
    "# 添加新欄位\n",
    "df['Valence'] = valence_scores\n",
    "df['Arousal'] = arousal_scores\n",
    "\n",
    "# 顯示統計結果\n",
    "print(f\"效價分數 - 平均值: {df['Valence'].mean():.3f}, 標準差: {df['Valence'].std():.3f}\")\n",
    "print(f\"喚醒度分數 - 平均值: {df['Arousal'].mean():.3f}, 標準差: {df['Arousal'].std():.3f}\")\n",
    "\n",
    "# 保存結果\n",
    "df.to_csv(\"amazon_2023_with_emotions.csv\", index=False)\n",
    "print(\"結果已保存到 amazon_2023_with_emotions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立 廣度(Breadth)欄位\n",
    "The number of topics of a review obtained from NMF topic modeling.\n",
    "<br>從 NMF 主題建模獲得的評價的主題數。\n",
    "\n",
    "#### 計算「廣度」（Breadth）的步驟\n",
    "\n",
    "1.  **主題建模（Topic Modeling）**\n",
    "    * 研究利用**非負矩陣分解（Non-Negative Matrix Factorization, NMF）**技術，自動從所有評論中發現共同主題。\n",
    "\n",
    "2.  **生成權重矩陣 W**\n",
    "    * NMF 會將評論的文本資料分解為兩個非負矩陣 $K$ 和 $W$ 的乘積。\n",
    "    * $W$ 矩陣是一個 $k$（主題數量）乘以 $n$（評論數量）的**權重矩陣**。\n",
    "    * $W$ 矩陣中的每個條目 $w_{ij}$ 代表**主題 $i$ 對於評論 $j$ 的權重**。這些 $w_{ij}$ 可被視為評論在特定主題上的「廣度分數」或「權重」。\n",
    "\n",
    "3.  **判斷主題是否被涵蓋**\n",
    "    * 為了確定每則評論的「廣度」，研究會評估每篇評論所涵蓋的總主題數量。\n",
    "    * 判斷一個主題 $i$ 是否被評論 $j$ 涵蓋的標準是：如果該評論 $j$ 在主題 $i$ 上的權重 ($w_{ij}$) **大於**該主題 $i$ 在所有評論中權重的中位數 ($median(w_{i1},...,w_{in})$)。\n",
    "\n",
    "4.  **計算評論的「廣度」**\n",
    "    * 一篇評論的「廣度」（Breadth）被定義為所有主題中，**符合上述條件（即 $w_{ij}$ 超過中位數）的主題數量之和**。\n",
    "    * 這透過一個**二元指示函數（Ind）**來計算，其公式為：\n",
    "        $$Breadth_j = \\sum_{i=1}^{k} Ind(w_{ij} > median(w_{i1},...,w_{in}))$$\n",
    "    * 這裡的 $Ind$ 函數，如果條件為真（即 $w_{ij}$ 大於中位數），則返回 1，否則返回 0。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChenChun\\AppData\\Local\\Temp\\ipykernel_26844\\4060925740.py:72: DtypeWarning: Columns (15,21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"amazon_2023_with_emotions.csv\")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamazon_2023_with_emotions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# 執行NMF廣度分析\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m df_result \u001b[38;5;241m=\u001b[39m \u001b[43mnmf_breadth_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# 存檔\u001b[39;00m\n\u001b[0;32m     78\u001b[0m df_result\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamazon_2023_with_breadth.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[33], line 67\u001b[0m, in \u001b[0;36mnmf_breadth_analysis\u001b[1;34m(df, n_topics)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# 逐一對應廣度分數\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_clean\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m---> 67\u001b[0m     df\u001b[38;5;241m.\u001b[39mat[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbreadth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mbreadth_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def nmf_breadth_analysis(df, n_topics=10):\n",
    "    \"\"\"\n",
    "    執行NMF廣度分析\n",
    "    \n",
    "    參數:\n",
    "    df: DataFrame，包含'title_processed'和'text_processed'欄位\n",
    "    n_topics: 主題數量，預設為10\n",
    "    \n",
    "    返回:\n",
    "    df: 新增'breadth'欄位的DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # 合併標題和文本（已經過預處理）\n",
    "    df['combined_text'] = df['title_processed'].fillna('') + ' ' + df['text_processed'].fillna('')\n",
    "    \n",
    "    # 移除空文本，但保留原始索引\n",
    "    mask = df['combined_text'].str.len() > 0\n",
    "    df_clean = df[mask].copy()\n",
    "    \n",
    "    if len(df_clean) == 0:\n",
    "        print(\"警告：沒有有效的文本資料進行分析\")\n",
    "        df['breadth'] = 0\n",
    "        return df\n",
    "    \n",
    "    # 建立文件-詞項矩陣 (TF-IDF)\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, min_df=2, max_df=0.95)\n",
    "    D = vectorizer.fit_transform(df_clean['combined_text'])\n",
    "    \n",
    "    # 執行NMF分解\n",
    "    # D矩陣: m(詞彙) × n(評論)\n",
    "    # K矩陣: m(詞彙) × k(主題) - 詞彙-主題矩陣\n",
    "    # W矩陣: k(主題) × n(評論) - 主題-評論權重矩陣\n",
    "    nmf = NMF(n_components=n_topics, random_state=42, max_iter=200)\n",
    "    W = nmf.fit_transform(D)  # W矩陣：k(主題) × n(評論)\n",
    "    K = nmf.components_      # K矩陣：k(主題) × m(詞彙)\n",
    "    \n",
    "    # 計算廣度 - 按照公式: Breadth_j = ∑k i=1 Ind(wij > median(wi1,…,win))\n",
    "    breadth_scores = []\n",
    "    \n",
    "    # 預先計算每個主題的中位數\n",
    "    topic_medians = []\n",
    "    for i in range(W.shape[0]):  # 對每個主題i\n",
    "        topic_weights = W[i, :]  # 主題i在所有評論中的權重\n",
    "        median_weight = np.median(topic_weights)\n",
    "        topic_medians.append(median_weight)\n",
    "    \n",
    "    # 計算每篇評論的廣度\n",
    "    for j in range(W.shape[1]):  # 對每篇評論j\n",
    "        breadth = 0\n",
    "        for i in range(W.shape[0]):  # 對每個主題i\n",
    "            # 二元指示函數 Ind(w_ij > median(w_i1,...,w_in))\n",
    "            if W[i, j] > topic_medians[i]:\n",
    "                breadth += 1  # 符合條件的主題計數+1\n",
    "        \n",
    "        breadth_scores.append(breadth)\n",
    "    \n",
    "    # 初始化廣度欄位\n",
    "    df['breadth'] = 0\n",
    "    \n",
    "    # 逐一對應廣度分數\n",
    "    for i, idx in enumerate(df_clean.index):\n",
    "        df.at[idx, 'breadth'] = breadth_scores[i]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 載入已預處理的資料\n",
    "df = pd.read_csv(\"amazon_2023_with_emotions.csv\")\n",
    "\n",
    "# 執行NMF廣度分析\n",
    "df_result = nmf_breadth_analysis(df, n_topics=10)\n",
    "\n",
    "# 存檔\n",
    "df_result.to_csv(\"amazon_2023_with_breadth.csv\", index=False)\n",
    "\n",
    "print(f\"分析完成！資料已存為 amazon_2023_with_breadth.csv\")\n",
    "print(f\"總共 {len(df_result)} 筆資料\")\n",
    "print(f\"廣度分數統計：\")\n",
    "print(df_result['breadth'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有效資料數量: 409482\n",
      "文件-詞項矩陣形狀: (409482, 1000)\n",
      "W矩陣形狀: (409482, 10)\n",
      "K矩陣形狀: (10, 1000)\n",
      "轉置後W矩陣形狀: (10, 409482)\n",
      "分析完成！資料已存為 amazon_2023_with_breadth.csv\n",
      "總共 409482 筆資料\n",
      "廣度分數統計：\n",
      "count    409482.000000\n",
      "mean          4.890784\n",
      "std           1.762957\n",
      "min           0.000000\n",
      "25%           4.000000\n",
      "50%           5.000000\n",
      "75%           6.000000\n",
      "max          10.000000\n",
      "Name: breadth, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def nmf_breadth_analysis(df, n_topics=10):\n",
    "    \"\"\"\n",
    "    執行NMF廣度分析\n",
    "    \n",
    "    參數:\n",
    "    df: DataFrame，包含'title_processed'和'text_processed'欄位\n",
    "    n_topics: 主題數量，預設為10\n",
    "    \n",
    "    返回:\n",
    "    df: 新增'breadth'欄位的DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # 合併標題和文本（已經過預處理）\n",
    "    df['combined_text'] = df['title_processed'].fillna('') + ' ' + df['text_processed'].fillna('')\n",
    "    \n",
    "    # 移除空文本，但保留原始索引\n",
    "    mask = df['combined_text'].str.len() > 0\n",
    "    df_clean = df[mask].copy()\n",
    "    \n",
    "    if len(df_clean) == 0:\n",
    "        print(\"警告：沒有有效的文本資料進行分析\")\n",
    "        df['breadth'] = 0\n",
    "        return df\n",
    "    \n",
    "    # 建立文件-詞項矩陣 (TF-IDF)\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, min_df=2, max_df=0.95)\n",
    "    D = vectorizer.fit_transform(df_clean['combined_text'])\n",
    "    \n",
    "    print(f\"有效資料數量: {len(df_clean)}\")\n",
    "    print(f\"文件-詞項矩陣形狀: {D.shape}\")\n",
    "    \n",
    "    if D.shape[0] == 0:\n",
    "        print(\"錯誤：沒有有效的詞彙進行分析\")\n",
    "        df['breadth'] = 0\n",
    "        return df\n",
    "    \n",
    "    # 執行NMF分解\n",
    "    # D矩陣: n(評論) × m(詞彙)\n",
    "    # W矩陣: n(評論) × k(主題) - fit_transform的結果\n",
    "    # K矩陣: k(主題) × m(詞彙) - components_\n",
    "    nmf = NMF(n_components=n_topics, random_state=42, max_iter=200)\n",
    "    W = nmf.fit_transform(D)  # W矩陣：n(評論) × k(主題)\n",
    "    K = nmf.components_      # K矩陣：k(主題) × m(詞彙)\n",
    "    \n",
    "    print(f\"W矩陣形狀: {W.shape}\")\n",
    "    print(f\"K矩陣形狀: {K.shape}\")\n",
    "    \n",
    "    # 轉置W矩陣以符合定義：k(主題) × n(評論)\n",
    "    W = W.T  # 現在W[i,j]表示主題i對評論j的權重\n",
    "    print(f\"轉置後W矩陣形狀: {W.shape}\")\n",
    "    \n",
    "    # 計算廣度 - 按照公式: Breadth_j = ∑k i=1 Ind(wij > median(wi1,…,win))\n",
    "    breadth_scores = []\n",
    "    \n",
    "    # 預先計算每個主題的中位數\n",
    "    topic_medians = []\n",
    "    for i in range(W.shape[0]):  # 對每個主題i\n",
    "        topic_weights = W[i, :]  # 主題i在所有評論中的權重\n",
    "        median_weight = np.median(topic_weights)\n",
    "        topic_medians.append(median_weight)\n",
    "    \n",
    "    # 計算每篇評論的廣度\n",
    "    for j in range(W.shape[1]):  # 對每篇評論j\n",
    "        breadth = 0\n",
    "        for i in range(W.shape[0]):  # 對每個主題i\n",
    "            # 二元指示函數 Ind(w_ij > median(w_i1,...,w_in))\n",
    "            if W[i, j] > topic_medians[i]:\n",
    "                breadth += 1  # 符合條件的主題計數+1\n",
    "        \n",
    "        breadth_scores.append(breadth)\n",
    "    \n",
    "    # 初始化廣度欄位\n",
    "    df['breadth'] = 0\n",
    "    \n",
    "    # 檢查長度是否匹配\n",
    "    if len(breadth_scores) != len(df_clean):\n",
    "        print(f\"警告：廣度分數數量 ({len(breadth_scores)}) 與有效資料數量 ({len(df_clean)}) 不匹配\")\n",
    "        return df\n",
    "    \n",
    "    # 逐一對應廣度分數\n",
    "    for i, idx in enumerate(df_clean.index):\n",
    "        if i < len(breadth_scores):  # 額外安全檢查\n",
    "            df.at[idx, 'breadth'] = breadth_scores[i]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # 載入已預處理的資料\n",
    "# df = pd.read_csv(\"amazon_2023_with_emotions.csv\")\n",
    "\n",
    "# 執行NMF廣度分析\n",
    "df_result = nmf_breadth_analysis(df, n_topics=10)\n",
    "\n",
    "# 存檔\n",
    "df_result.to_csv(\"amazon_2023_with_breadth.csv\", index=False)\n",
    "\n",
    "print(f\"分析完成！資料已存為 amazon_2023_with_breadth.csv\")\n",
    "print(f\"總共 {len(df_result)} 筆資料\")\n",
    "print(f\"廣度分數統計：\")\n",
    "print(df_result['breadth'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可讀性分析 (Readability Analysis)\n",
    "這是透過計算評論的Flesch閱讀易性指數 (Flesch Reading Ease Index) 來衡量其可讀性。這是一個客觀指標，與評論的情緒或主題內容無直接關係，而是關注文本的語言結構。\n",
    "\n",
    "可讀性分析是透過計算評論的 Flesch 閱讀易性指數 (Flesch Reading Ease Index) 來衡量的<br>\n",
    "其計算公式如下： 206.835 − 1.015 ( 總字數 / 總句數 ) − 84.6 ( 總音節數 / 總字數 )<br>\n",
    "在這個公式中：\n",
    "- 總字數 (total words)：指評論中的詞彙總數。\n",
    "- 總句數 (total sentences)：指評論中的句子總數。\n",
    "- 總音節數 (total syllables)：指評論中所有詞彙的音節總數。\n",
    "研究指出，Flesch 閱讀易性指數的數值越高，代表該評論的可讀性越好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK punkt_tab tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ChenChun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "C:\\Users\\ChenChun\\AppData\\Local\\Temp\\ipykernel_26844\\137194136.py:102: DtypeWarning: Columns (15,21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "載入資料：409482 筆\n",
      "正在計算Flesch可讀性指數...\n",
      "進度: 0/409482 (0.0%)\n",
      "進度: 1000/409482 (0.2%)\n",
      "進度: 2000/409482 (0.5%)\n",
      "進度: 3000/409482 (0.7%)\n",
      "進度: 4000/409482 (1.0%)\n",
      "進度: 5000/409482 (1.2%)\n",
      "進度: 6000/409482 (1.5%)\n",
      "進度: 7000/409482 (1.7%)\n",
      "進度: 8000/409482 (2.0%)\n",
      "進度: 9000/409482 (2.2%)\n",
      "進度: 10000/409482 (2.4%)\n",
      "進度: 11000/409482 (2.7%)\n",
      "進度: 12000/409482 (2.9%)\n",
      "進度: 13000/409482 (3.2%)\n",
      "進度: 14000/409482 (3.4%)\n",
      "進度: 15000/409482 (3.7%)\n",
      "進度: 16000/409482 (3.9%)\n",
      "進度: 17000/409482 (4.2%)\n",
      "進度: 18000/409482 (4.4%)\n",
      "進度: 19000/409482 (4.6%)\n",
      "進度: 20000/409482 (4.9%)\n",
      "進度: 21000/409482 (5.1%)\n",
      "進度: 22000/409482 (5.4%)\n",
      "進度: 23000/409482 (5.6%)\n",
      "進度: 24000/409482 (5.9%)\n",
      "進度: 25000/409482 (6.1%)\n",
      "進度: 26000/409482 (6.3%)\n",
      "進度: 27000/409482 (6.6%)\n",
      "進度: 28000/409482 (6.8%)\n",
      "進度: 29000/409482 (7.1%)\n",
      "進度: 30000/409482 (7.3%)\n",
      "進度: 31000/409482 (7.6%)\n",
      "進度: 32000/409482 (7.8%)\n",
      "進度: 33000/409482 (8.1%)\n",
      "進度: 34000/409482 (8.3%)\n",
      "進度: 35000/409482 (8.5%)\n",
      "進度: 36000/409482 (8.8%)\n",
      "進度: 37000/409482 (9.0%)\n",
      "進度: 38000/409482 (9.3%)\n",
      "進度: 39000/409482 (9.5%)\n",
      "進度: 40000/409482 (9.8%)\n",
      "進度: 41000/409482 (10.0%)\n",
      "進度: 42000/409482 (10.3%)\n",
      "進度: 43000/409482 (10.5%)\n",
      "進度: 44000/409482 (10.7%)\n",
      "進度: 45000/409482 (11.0%)\n",
      "進度: 46000/409482 (11.2%)\n",
      "進度: 47000/409482 (11.5%)\n",
      "進度: 48000/409482 (11.7%)\n",
      "進度: 49000/409482 (12.0%)\n",
      "進度: 50000/409482 (12.2%)\n",
      "進度: 51000/409482 (12.5%)\n",
      "進度: 52000/409482 (12.7%)\n",
      "進度: 53000/409482 (12.9%)\n",
      "進度: 54000/409482 (13.2%)\n",
      "進度: 55000/409482 (13.4%)\n",
      "進度: 56000/409482 (13.7%)\n",
      "進度: 57000/409482 (13.9%)\n",
      "進度: 58000/409482 (14.2%)\n",
      "進度: 59000/409482 (14.4%)\n",
      "進度: 60000/409482 (14.7%)\n",
      "進度: 61000/409482 (14.9%)\n",
      "進度: 62000/409482 (15.1%)\n",
      "進度: 63000/409482 (15.4%)\n",
      "進度: 64000/409482 (15.6%)\n",
      "進度: 65000/409482 (15.9%)\n",
      "進度: 66000/409482 (16.1%)\n",
      "進度: 67000/409482 (16.4%)\n",
      "進度: 68000/409482 (16.6%)\n",
      "進度: 69000/409482 (16.9%)\n",
      "進度: 70000/409482 (17.1%)\n",
      "進度: 71000/409482 (17.3%)\n",
      "進度: 72000/409482 (17.6%)\n",
      "進度: 73000/409482 (17.8%)\n",
      "進度: 74000/409482 (18.1%)\n",
      "進度: 75000/409482 (18.3%)\n",
      "進度: 76000/409482 (18.6%)\n",
      "進度: 77000/409482 (18.8%)\n",
      "進度: 78000/409482 (19.0%)\n",
      "進度: 79000/409482 (19.3%)\n",
      "進度: 80000/409482 (19.5%)\n",
      "進度: 81000/409482 (19.8%)\n",
      "進度: 82000/409482 (20.0%)\n",
      "進度: 83000/409482 (20.3%)\n",
      "進度: 84000/409482 (20.5%)\n",
      "進度: 85000/409482 (20.8%)\n",
      "進度: 86000/409482 (21.0%)\n",
      "進度: 87000/409482 (21.2%)\n",
      "進度: 88000/409482 (21.5%)\n",
      "進度: 89000/409482 (21.7%)\n",
      "進度: 90000/409482 (22.0%)\n",
      "進度: 91000/409482 (22.2%)\n",
      "進度: 92000/409482 (22.5%)\n",
      "進度: 93000/409482 (22.7%)\n",
      "進度: 94000/409482 (23.0%)\n",
      "進度: 95000/409482 (23.2%)\n",
      "進度: 96000/409482 (23.4%)\n",
      "進度: 97000/409482 (23.7%)\n",
      "進度: 98000/409482 (23.9%)\n",
      "進度: 99000/409482 (24.2%)\n",
      "進度: 100000/409482 (24.4%)\n",
      "進度: 101000/409482 (24.7%)\n",
      "進度: 102000/409482 (24.9%)\n",
      "進度: 103000/409482 (25.2%)\n",
      "進度: 104000/409482 (25.4%)\n",
      "進度: 105000/409482 (25.6%)\n",
      "進度: 106000/409482 (25.9%)\n",
      "進度: 107000/409482 (26.1%)\n",
      "進度: 108000/409482 (26.4%)\n",
      "進度: 109000/409482 (26.6%)\n",
      "進度: 110000/409482 (26.9%)\n",
      "進度: 111000/409482 (27.1%)\n",
      "進度: 112000/409482 (27.4%)\n",
      "進度: 113000/409482 (27.6%)\n",
      "進度: 114000/409482 (27.8%)\n",
      "進度: 115000/409482 (28.1%)\n",
      "進度: 116000/409482 (28.3%)\n",
      "進度: 117000/409482 (28.6%)\n",
      "進度: 118000/409482 (28.8%)\n",
      "進度: 119000/409482 (29.1%)\n",
      "進度: 120000/409482 (29.3%)\n",
      "進度: 121000/409482 (29.5%)\n",
      "進度: 122000/409482 (29.8%)\n",
      "進度: 123000/409482 (30.0%)\n",
      "進度: 124000/409482 (30.3%)\n",
      "進度: 125000/409482 (30.5%)\n",
      "進度: 126000/409482 (30.8%)\n",
      "進度: 127000/409482 (31.0%)\n",
      "進度: 128000/409482 (31.3%)\n",
      "進度: 129000/409482 (31.5%)\n",
      "進度: 130000/409482 (31.7%)\n",
      "進度: 131000/409482 (32.0%)\n",
      "進度: 132000/409482 (32.2%)\n",
      "進度: 133000/409482 (32.5%)\n",
      "進度: 134000/409482 (32.7%)\n",
      "進度: 135000/409482 (33.0%)\n",
      "進度: 136000/409482 (33.2%)\n",
      "進度: 137000/409482 (33.5%)\n",
      "進度: 138000/409482 (33.7%)\n",
      "進度: 139000/409482 (33.9%)\n",
      "進度: 140000/409482 (34.2%)\n",
      "進度: 141000/409482 (34.4%)\n",
      "進度: 142000/409482 (34.7%)\n",
      "進度: 143000/409482 (34.9%)\n",
      "進度: 144000/409482 (35.2%)\n",
      "進度: 145000/409482 (35.4%)\n",
      "進度: 146000/409482 (35.7%)\n",
      "進度: 147000/409482 (35.9%)\n",
      "進度: 148000/409482 (36.1%)\n",
      "進度: 149000/409482 (36.4%)\n",
      "進度: 150000/409482 (36.6%)\n",
      "進度: 151000/409482 (36.9%)\n",
      "進度: 152000/409482 (37.1%)\n",
      "進度: 153000/409482 (37.4%)\n",
      "進度: 154000/409482 (37.6%)\n",
      "進度: 155000/409482 (37.9%)\n",
      "進度: 156000/409482 (38.1%)\n",
      "進度: 157000/409482 (38.3%)\n",
      "進度: 158000/409482 (38.6%)\n",
      "進度: 159000/409482 (38.8%)\n",
      "進度: 160000/409482 (39.1%)\n",
      "進度: 161000/409482 (39.3%)\n",
      "進度: 162000/409482 (39.6%)\n",
      "進度: 163000/409482 (39.8%)\n",
      "進度: 164000/409482 (40.1%)\n",
      "進度: 165000/409482 (40.3%)\n",
      "進度: 166000/409482 (40.5%)\n",
      "進度: 167000/409482 (40.8%)\n",
      "進度: 168000/409482 (41.0%)\n",
      "進度: 169000/409482 (41.3%)\n",
      "進度: 170000/409482 (41.5%)\n",
      "進度: 171000/409482 (41.8%)\n",
      "進度: 172000/409482 (42.0%)\n",
      "進度: 173000/409482 (42.2%)\n",
      "進度: 174000/409482 (42.5%)\n",
      "進度: 175000/409482 (42.7%)\n",
      "進度: 176000/409482 (43.0%)\n",
      "進度: 177000/409482 (43.2%)\n",
      "進度: 178000/409482 (43.5%)\n",
      "進度: 179000/409482 (43.7%)\n",
      "進度: 180000/409482 (44.0%)\n",
      "進度: 181000/409482 (44.2%)\n",
      "進度: 182000/409482 (44.4%)\n",
      "進度: 183000/409482 (44.7%)\n",
      "進度: 184000/409482 (44.9%)\n",
      "進度: 185000/409482 (45.2%)\n",
      "進度: 186000/409482 (45.4%)\n",
      "進度: 187000/409482 (45.7%)\n",
      "進度: 188000/409482 (45.9%)\n",
      "進度: 189000/409482 (46.2%)\n",
      "進度: 190000/409482 (46.4%)\n",
      "進度: 191000/409482 (46.6%)\n",
      "進度: 192000/409482 (46.9%)\n",
      "進度: 193000/409482 (47.1%)\n",
      "進度: 194000/409482 (47.4%)\n",
      "進度: 195000/409482 (47.6%)\n",
      "進度: 196000/409482 (47.9%)\n",
      "進度: 197000/409482 (48.1%)\n",
      "進度: 198000/409482 (48.4%)\n",
      "進度: 199000/409482 (48.6%)\n",
      "進度: 200000/409482 (48.8%)\n",
      "進度: 201000/409482 (49.1%)\n",
      "進度: 202000/409482 (49.3%)\n",
      "進度: 203000/409482 (49.6%)\n",
      "進度: 204000/409482 (49.8%)\n",
      "進度: 205000/409482 (50.1%)\n",
      "進度: 206000/409482 (50.3%)\n",
      "進度: 207000/409482 (50.6%)\n",
      "進度: 208000/409482 (50.8%)\n",
      "進度: 209000/409482 (51.0%)\n",
      "進度: 210000/409482 (51.3%)\n",
      "進度: 211000/409482 (51.5%)\n",
      "進度: 212000/409482 (51.8%)\n",
      "進度: 213000/409482 (52.0%)\n",
      "進度: 214000/409482 (52.3%)\n",
      "進度: 215000/409482 (52.5%)\n",
      "進度: 216000/409482 (52.7%)\n",
      "進度: 217000/409482 (53.0%)\n",
      "進度: 218000/409482 (53.2%)\n",
      "進度: 219000/409482 (53.5%)\n",
      "進度: 220000/409482 (53.7%)\n",
      "進度: 221000/409482 (54.0%)\n",
      "進度: 222000/409482 (54.2%)\n",
      "進度: 223000/409482 (54.5%)\n",
      "進度: 224000/409482 (54.7%)\n",
      "進度: 225000/409482 (54.9%)\n",
      "進度: 226000/409482 (55.2%)\n",
      "進度: 227000/409482 (55.4%)\n",
      "進度: 228000/409482 (55.7%)\n",
      "進度: 229000/409482 (55.9%)\n",
      "進度: 230000/409482 (56.2%)\n",
      "進度: 231000/409482 (56.4%)\n",
      "進度: 232000/409482 (56.7%)\n",
      "進度: 233000/409482 (56.9%)\n",
      "進度: 234000/409482 (57.1%)\n",
      "進度: 235000/409482 (57.4%)\n",
      "進度: 236000/409482 (57.6%)\n",
      "進度: 237000/409482 (57.9%)\n",
      "進度: 238000/409482 (58.1%)\n",
      "進度: 239000/409482 (58.4%)\n",
      "進度: 240000/409482 (58.6%)\n",
      "進度: 241000/409482 (58.9%)\n",
      "進度: 242000/409482 (59.1%)\n",
      "進度: 243000/409482 (59.3%)\n",
      "進度: 244000/409482 (59.6%)\n",
      "進度: 245000/409482 (59.8%)\n",
      "進度: 246000/409482 (60.1%)\n",
      "進度: 247000/409482 (60.3%)\n",
      "進度: 248000/409482 (60.6%)\n",
      "進度: 249000/409482 (60.8%)\n",
      "進度: 250000/409482 (61.1%)\n",
      "進度: 251000/409482 (61.3%)\n",
      "進度: 252000/409482 (61.5%)\n",
      "進度: 253000/409482 (61.8%)\n",
      "進度: 254000/409482 (62.0%)\n",
      "進度: 255000/409482 (62.3%)\n",
      "進度: 256000/409482 (62.5%)\n",
      "進度: 257000/409482 (62.8%)\n",
      "進度: 258000/409482 (63.0%)\n",
      "進度: 259000/409482 (63.3%)\n",
      "進度: 260000/409482 (63.5%)\n",
      "進度: 261000/409482 (63.7%)\n",
      "進度: 262000/409482 (64.0%)\n",
      "進度: 263000/409482 (64.2%)\n",
      "進度: 264000/409482 (64.5%)\n",
      "進度: 265000/409482 (64.7%)\n",
      "進度: 266000/409482 (65.0%)\n",
      "進度: 267000/409482 (65.2%)\n",
      "進度: 268000/409482 (65.4%)\n",
      "進度: 269000/409482 (65.7%)\n",
      "進度: 270000/409482 (65.9%)\n",
      "進度: 271000/409482 (66.2%)\n",
      "進度: 272000/409482 (66.4%)\n",
      "進度: 273000/409482 (66.7%)\n",
      "進度: 274000/409482 (66.9%)\n",
      "進度: 275000/409482 (67.2%)\n",
      "進度: 276000/409482 (67.4%)\n",
      "進度: 277000/409482 (67.6%)\n",
      "進度: 278000/409482 (67.9%)\n",
      "進度: 279000/409482 (68.1%)\n",
      "進度: 280000/409482 (68.4%)\n",
      "進度: 281000/409482 (68.6%)\n",
      "進度: 282000/409482 (68.9%)\n",
      "進度: 283000/409482 (69.1%)\n",
      "進度: 284000/409482 (69.4%)\n",
      "進度: 285000/409482 (69.6%)\n",
      "進度: 286000/409482 (69.8%)\n",
      "進度: 287000/409482 (70.1%)\n",
      "進度: 288000/409482 (70.3%)\n",
      "進度: 289000/409482 (70.6%)\n",
      "進度: 290000/409482 (70.8%)\n",
      "進度: 291000/409482 (71.1%)\n",
      "進度: 292000/409482 (71.3%)\n",
      "進度: 293000/409482 (71.6%)\n",
      "進度: 294000/409482 (71.8%)\n",
      "進度: 295000/409482 (72.0%)\n",
      "進度: 296000/409482 (72.3%)\n",
      "進度: 297000/409482 (72.5%)\n",
      "進度: 298000/409482 (72.8%)\n",
      "進度: 299000/409482 (73.0%)\n",
      "進度: 300000/409482 (73.3%)\n",
      "進度: 301000/409482 (73.5%)\n",
      "進度: 302000/409482 (73.8%)\n",
      "進度: 303000/409482 (74.0%)\n",
      "進度: 304000/409482 (74.2%)\n",
      "進度: 305000/409482 (74.5%)\n",
      "進度: 306000/409482 (74.7%)\n",
      "進度: 307000/409482 (75.0%)\n",
      "進度: 308000/409482 (75.2%)\n",
      "進度: 309000/409482 (75.5%)\n",
      "進度: 310000/409482 (75.7%)\n",
      "進度: 311000/409482 (75.9%)\n",
      "進度: 312000/409482 (76.2%)\n",
      "進度: 313000/409482 (76.4%)\n",
      "進度: 314000/409482 (76.7%)\n",
      "進度: 315000/409482 (76.9%)\n",
      "進度: 316000/409482 (77.2%)\n",
      "進度: 317000/409482 (77.4%)\n",
      "進度: 318000/409482 (77.7%)\n",
      "進度: 319000/409482 (77.9%)\n",
      "進度: 320000/409482 (78.1%)\n",
      "進度: 321000/409482 (78.4%)\n",
      "進度: 322000/409482 (78.6%)\n",
      "進度: 323000/409482 (78.9%)\n",
      "進度: 324000/409482 (79.1%)\n",
      "進度: 325000/409482 (79.4%)\n",
      "進度: 326000/409482 (79.6%)\n",
      "進度: 327000/409482 (79.9%)\n",
      "進度: 328000/409482 (80.1%)\n",
      "進度: 329000/409482 (80.3%)\n",
      "進度: 330000/409482 (80.6%)\n",
      "進度: 331000/409482 (80.8%)\n",
      "進度: 332000/409482 (81.1%)\n",
      "進度: 333000/409482 (81.3%)\n",
      "進度: 334000/409482 (81.6%)\n",
      "進度: 335000/409482 (81.8%)\n",
      "進度: 336000/409482 (82.1%)\n",
      "進度: 337000/409482 (82.3%)\n",
      "進度: 338000/409482 (82.5%)\n",
      "進度: 339000/409482 (82.8%)\n",
      "進度: 340000/409482 (83.0%)\n",
      "進度: 341000/409482 (83.3%)\n",
      "進度: 342000/409482 (83.5%)\n",
      "進度: 343000/409482 (83.8%)\n",
      "進度: 344000/409482 (84.0%)\n",
      "進度: 345000/409482 (84.3%)\n",
      "進度: 346000/409482 (84.5%)\n",
      "進度: 347000/409482 (84.7%)\n",
      "進度: 348000/409482 (85.0%)\n",
      "進度: 349000/409482 (85.2%)\n",
      "進度: 350000/409482 (85.5%)\n",
      "進度: 351000/409482 (85.7%)\n",
      "進度: 352000/409482 (86.0%)\n",
      "進度: 353000/409482 (86.2%)\n",
      "進度: 354000/409482 (86.5%)\n",
      "進度: 355000/409482 (86.7%)\n",
      "進度: 356000/409482 (86.9%)\n",
      "進度: 357000/409482 (87.2%)\n",
      "進度: 358000/409482 (87.4%)\n",
      "進度: 359000/409482 (87.7%)\n",
      "進度: 360000/409482 (87.9%)\n",
      "進度: 361000/409482 (88.2%)\n",
      "進度: 362000/409482 (88.4%)\n",
      "進度: 363000/409482 (88.6%)\n",
      "進度: 364000/409482 (88.9%)\n",
      "進度: 365000/409482 (89.1%)\n",
      "進度: 366000/409482 (89.4%)\n",
      "進度: 367000/409482 (89.6%)\n",
      "進度: 368000/409482 (89.9%)\n",
      "進度: 369000/409482 (90.1%)\n",
      "進度: 370000/409482 (90.4%)\n",
      "進度: 371000/409482 (90.6%)\n",
      "進度: 372000/409482 (90.8%)\n",
      "進度: 373000/409482 (91.1%)\n",
      "進度: 374000/409482 (91.3%)\n",
      "進度: 375000/409482 (91.6%)\n",
      "進度: 376000/409482 (91.8%)\n",
      "進度: 377000/409482 (92.1%)\n",
      "進度: 378000/409482 (92.3%)\n",
      "進度: 379000/409482 (92.6%)\n",
      "進度: 380000/409482 (92.8%)\n",
      "進度: 381000/409482 (93.0%)\n",
      "進度: 382000/409482 (93.3%)\n",
      "進度: 383000/409482 (93.5%)\n",
      "進度: 384000/409482 (93.8%)\n",
      "進度: 385000/409482 (94.0%)\n",
      "進度: 386000/409482 (94.3%)\n",
      "進度: 387000/409482 (94.5%)\n",
      "進度: 388000/409482 (94.8%)\n",
      "進度: 389000/409482 (95.0%)\n",
      "進度: 390000/409482 (95.2%)\n",
      "進度: 391000/409482 (95.5%)\n",
      "進度: 392000/409482 (95.7%)\n",
      "進度: 393000/409482 (96.0%)\n",
      "進度: 394000/409482 (96.2%)\n",
      "進度: 395000/409482 (96.5%)\n",
      "進度: 396000/409482 (96.7%)\n",
      "進度: 397000/409482 (97.0%)\n",
      "進度: 398000/409482 (97.2%)\n",
      "進度: 399000/409482 (97.4%)\n",
      "進度: 400000/409482 (97.7%)\n",
      "進度: 401000/409482 (97.9%)\n",
      "進度: 402000/409482 (98.2%)\n",
      "進度: 403000/409482 (98.4%)\n",
      "進度: 404000/409482 (98.7%)\n",
      "進度: 405000/409482 (98.9%)\n",
      "進度: 406000/409482 (99.1%)\n",
      "進度: 407000/409482 (99.4%)\n",
      "進度: 408000/409482 (99.6%)\n",
      "進度: 409000/409482 (99.9%)\n",
      "分析完成！最新資料已存為 amazon_2023_with_breadth_readability.csv\n",
      "可讀性分數統計：\n",
      "count    409482.000000\n",
      "mean         27.722906\n",
      "std          68.116568\n",
      "min       -2284.780000\n",
      "25%          17.390000\n",
      "50%          43.200000\n",
      "75%          61.890000\n",
      "max         121.220000\n",
      "Name: readability, dtype: float64\n",
      "\n",
      "可讀性分數分布：\n",
      "0-30: 非常困難\n",
      "30-50: 困難\n",
      "50-60: 相當困難\n",
      "60-70: 標準\n",
      "70-80: 相當容易\n",
      "80-90: 容易\n",
      "90-100: 非常容易\n",
      "\n",
      "各難度等級分布：\n",
      "readability_level\n",
      "困難      100912\n",
      "非常困難     69762\n",
      "相當困難     53981\n",
      "標準       47233\n",
      "相當容易     29976\n",
      "容易       18173\n",
      "非常容易      9657\n",
      "Name: count, dtype: int64\n",
      "\n",
      "總共處理 409482 筆資料\n",
      "平均可讀性分數: 27.72\n",
      "可讀性分數中位數: 43.20\n",
      "可讀性分數標準差: 68.12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# 更完整的NLTK資源下載\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK punkt_tab tokenizer...\")\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "def count_syllables(word):\n",
    "    \"\"\"計算單詞音節數\"\"\"\n",
    "    word = word.lower()\n",
    "    if word == '':\n",
    "        return 0\n",
    "    \n",
    "    # 移除標點符號\n",
    "    word = re.sub(r'[^a-z]', '', word)\n",
    "    if not word:\n",
    "        return 0\n",
    "    \n",
    "    # 計算母音數量作為音節估算\n",
    "    vowels = 'aeiouy'\n",
    "    syllables = 0\n",
    "    prev_was_vowel = False\n",
    "    \n",
    "    for char in word:\n",
    "        is_vowel = char in vowels\n",
    "        if is_vowel and not prev_was_vowel:\n",
    "            syllables += 1\n",
    "        prev_was_vowel = is_vowel\n",
    "    \n",
    "    # 特殊規則：以e結尾的單詞通常不發音\n",
    "    if word.endswith('e') and syllables > 1:\n",
    "        syllables -= 1\n",
    "    \n",
    "    # 每個單詞至少有1個音節\n",
    "    return max(1, syllables)\n",
    "\n",
    "def simple_sentence_tokenize(text):\n",
    "    \"\"\"簡單的句子分割（備用方案）\"\"\"\n",
    "    # 使用正則表達式分割句子\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    # 移除空白句子\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "def simple_word_tokenize(text):\n",
    "    \"\"\"簡單的單詞分割（備用方案）\"\"\"\n",
    "    # 使用正則表達式提取單詞\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "    return words\n",
    "\n",
    "def calculate_flesch_score(text):\n",
    "    \"\"\"\n",
    "    計算Flesch閱讀易性指數\n",
    "    公式: 206.835 - 1.015(總字數/總句數) - 84.6(總音節數/總字數)\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        # 嘗試使用NLTK進行句子分割\n",
    "        sentences = sent_tokenize(text)\n",
    "        words = word_tokenize(text)\n",
    "        # 只保留字母組成的詞\n",
    "        words = [word for word in words if re.match(r'^[a-zA-Z]+$', word)]\n",
    "    except:\n",
    "        # 如果NLTK失敗，使用備用方案\n",
    "        print(\"使用備用分詞方案...\")\n",
    "        sentences = simple_sentence_tokenize(text)\n",
    "        words = simple_word_tokenize(text)\n",
    "    \n",
    "    total_sentences = len(sentences)\n",
    "    total_words = len(words)\n",
    "    \n",
    "    if total_sentences == 0 or total_words == 0:\n",
    "        return 0\n",
    "    \n",
    "    # 計算總音節數\n",
    "    total_syllables = sum(count_syllables(word) for word in words)\n",
    "    \n",
    "    # 計算Flesch閱讀易性指數\n",
    "    flesch_score = 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (total_syllables / total_words)\n",
    "    \n",
    "    return round(flesch_score, 2)\n",
    "\n",
    "def add_readability_analysis(csv_file):\n",
    "    \"\"\"\n",
    "    為CSV文件添加可讀性分析\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 載入資料\n",
    "        df = pd.read_csv(csv_file)\n",
    "        print(f\"載入資料：{len(df)} 筆\")\n",
    "        \n",
    "        # 合併title_processed和text_processed進行分析\n",
    "        df['combined_text'] = df['title_processed'].fillna('') + ' ' + df['text_processed'].fillna('')\n",
    "        \n",
    "        # 計算Flesch可讀性指數\n",
    "        print(\"正在計算Flesch可讀性指數...\")\n",
    "        \n",
    "        # 添加進度顯示\n",
    "        total_rows = len(df)\n",
    "        readability_scores = []\n",
    "        \n",
    "        for i, text in enumerate(df['combined_text']):\n",
    "            if i % 1000 == 0:  # 每1000筆顯示進度\n",
    "                print(f\"進度: {i}/{total_rows} ({i/total_rows*100:.1f}%)\")\n",
    "            \n",
    "            score = calculate_flesch_score(text)\n",
    "            readability_scores.append(score)\n",
    "        \n",
    "        df['readability'] = readability_scores\n",
    "        \n",
    "        # 移除臨時欄位\n",
    "        df = df.drop('combined_text', axis=1)\n",
    "        \n",
    "        # 存檔\n",
    "        output_file = \"amazon_2023_with_breadth_readability.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"分析完成！最新資料已存為 {output_file}\")\n",
    "        print(f\"可讀性分數統計：\")\n",
    "        print(df['readability'].describe())\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"錯誤：找不到檔案 {csv_file}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"處理過程中發生錯誤：{str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 執行分析\n",
    "if __name__ == \"__main__\":\n",
    "    df_result = add_readability_analysis(\"amazon_2023_with_breadth.csv\")\n",
    "    \n",
    "    if df_result is not None:\n",
    "        # 顯示可讀性分數分布\n",
    "        print(\"\\n可讀性分數分布：\")\n",
    "        print(\"0-30: 非常困難\")\n",
    "        print(\"30-50: 困難\") \n",
    "        print(\"50-60: 相當困難\")\n",
    "        print(\"60-70: 標準\")\n",
    "        print(\"70-80: 相當容易\")\n",
    "        print(\"80-90: 容易\")\n",
    "        print(\"90-100: 非常容易\")\n",
    "        \n",
    "        # 統計各區間的數量\n",
    "        bins = [0, 30, 50, 60, 70, 80, 90, 100]\n",
    "        labels = ['非常困難', '困難', '相當困難', '標準', '相當容易', '容易', '非常容易']\n",
    "        df_result['readability_level'] = pd.cut(df_result['readability'], bins=bins, labels=labels, include_lowest=True)\n",
    "        \n",
    "        print(\"\\n各難度等級分布：\")\n",
    "        print(df_result['readability_level'].value_counts())\n",
    "        \n",
    "        # 顯示一些統計資訊\n",
    "        print(f\"\\n總共處理 {len(df_result)} 筆資料\")\n",
    "        print(f\"平均可讀性分數: {df_result['readability'].mean():.2f}\")\n",
    "        print(f\"可讀性分數中位數: {df_result['readability'].median():.2f}\")\n",
    "        print(f\"可讀性分數標準差: {df_result['readability'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 類別分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 建立資料\n",
    "data = {\n",
    "    '類別': ['Books(實體書)', 'Buy a Kindle', 'Toys & Games', 'Amazon Home', 'Office Products',\n",
    "            'Musical Instruments', 'Arts, Crafts & Sewing', 'Cell Phones & Accessories', \n",
    "            'Sports & Outdoors', 'All Electronics', 'AMAZON FASHION', 'Health & Personal Care',\n",
    "            'Industrial & Scientific', 'Automotive', 'Computers', 'Tools & Home Improvement',\n",
    "            'Home Audio & Theater', 'Camera & Photo', 'Car Electronics', 'Movies & TV',\n",
    "            'Portable Audio & Accessories', 'Video Games', 'Amazon Devices', 'All Beauty',\n",
    "            'GPS & Navigation', 'Fire Phone', 'Baby', 'Pet Supplies', 'Appliances',\n",
    "            'Apple Products', 'Software', 'Amazon Fire TV', 'Premium Beauty', 'Grocery',\n",
    "            'Handmade', 'Digital Music', 'Appstore for Android', 'Collectible Coins', 'Sports Collectibles'],\n",
    "    '資訊透明度': [5,5,4,4,5,3,4,5,4,5,3,3,5,4,5,4,5,5,5,5,5,4,5,3,5,5,3,4,5,5,5,5,3,3,3,5,5,3,3],\n",
    "    '質量可判斷': [4,5,3,4,5,2,3,5,3,5,2,2,5,3,5,4,5,5,5,4,5,3,5,2,5,5,2,3,5,5,4,5,2,2,2,4,4,2,2],\n",
    "    '體驗依賴': [2,1,3,2,1,5,3,1,4,1,5,4,1,3,1,2,2,1,1,2,1,4,1,5,1,1,5,4,1,1,2,1,5,5,5,2,2,5,5],\n",
    "    '評論依賴': [3,2,3,2,2,4,3,2,3,2,4,4,2,3,2,2,3,2,2,3,2,4,2,4,2,2,4,3,2,2,3,2,4,4,4,3,3,4,4],\n",
    "    '使用後驗證': [2,1,3,2,1,5,3,1,4,1,5,5,1,3,1,2,2,1,1,2,1,4,1,5,1,1,5,4,1,1,2,1,5,5,5,2,2,5,5],\n",
    "    '最終分類': ['搜尋品','搜尋品','混合型','搜尋品','搜尋品','經驗品','混合型','搜尋品','混合型','搜尋品',\n",
    "               '經驗品','經驗品','搜尋品','混合型','搜尋品','搜尋品','搜尋品','搜尋品','搜尋品','搜尋品',\n",
    "               '搜尋品','混合型','搜尋品','經驗品','搜尋品','搜尋品','經驗品','混合型','搜尋品','搜尋品',\n",
    "               '搜尋品','搜尋品','經驗品','經驗品','經驗品','搜尋品','搜尋品','經驗品','經驗品']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 設定字體\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 建立視覺化\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# 1. Radar Chart - Average indicators by classification\n",
    "categories = ['Search Goods', 'Experience Goods', 'Hybrid']\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "\n",
    "# Calculate average values for each classification\n",
    "avg_scores = df.groupby('最終分類')[['資訊透明度', '質量可判斷', '體驗依賴', '評論依賴', '使用後驗證']].mean()\n",
    "\n",
    "# Map Chinese categories to English\n",
    "category_mapping = {'搜尋品': 'Search Goods', '經驗品': 'Experience Goods', '混合型': 'Hybrid'}\n",
    "avg_scores.index = avg_scores.index.map(category_mapping)\n",
    "\n",
    "# 雷達圖設定\n",
    "angles = np.linspace(0, 2*np.pi, 5, endpoint=False)\n",
    "angles = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "# 重新建立極座標子圖\n",
    "ax1 = plt.subplot(221, projection='polar')\n",
    "ax1.set_theta_offset(np.pi / 2)\n",
    "ax1.set_theta_direction(-1)\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    values = avg_scores.loc[category].values\n",
    "    values = np.concatenate((values, [values[0]]))\n",
    "    ax1.plot(angles, values, 'o-', linewidth=2, label=category, color=colors[i])\n",
    "    ax1.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "\n",
    "ax1.set_xticks(angles[:-1])\n",
    "ax1.set_xticklabels(['Info Transparency', 'Quality Judgeable', 'Experience Dependent', 'Review Dependent', 'Post-use Verification'])\n",
    "ax1.set_ylim(0, 5)\n",
    "ax1.set_title('Average Indicators Radar Chart by Classification', size=14, weight='bold', pad=20)\n",
    "ax1.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "ax1.grid(True)\n",
    "\n",
    "# 2. Scatter Plot - Info Transparency vs Experience Dependent\n",
    "category_colors = {'Search Goods': '#2E86AB', 'Experience Goods': '#A23B72', 'Hybrid': '#F18F01'}\n",
    "# Create English category column for plotting\n",
    "df['Category_EN'] = df['最終分類'].map(category_mapping)\n",
    "\n",
    "for category in categories:\n",
    "    mask = df['Category_EN'] == category\n",
    "    ax2.scatter(df[mask]['資訊透明度'], df[mask]['體驗依賴'], \n",
    "               c=category_colors[category], label=category, alpha=0.7, s=80)\n",
    "\n",
    "ax2.set_xlabel('Info Transparency', fontsize=12)\n",
    "ax2.set_ylabel('Experience Dependent', fontsize=12)\n",
    "ax2.set_title('Info Transparency vs Experience Dependent', fontsize=14, weight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0.5, 5.5)\n",
    "ax2.set_ylim(0.5, 5.5)\n",
    "\n",
    "# 3. Heatmap - All indicators\n",
    "metrics = ['資訊透明度', '質量可判斷', '體驗依賴', '評論依賴', '使用後驗證']\n",
    "metrics_en = ['Info Transparency', 'Quality Judgeable', 'Experience Dependent', 'Review Dependent', 'Post-use Verification']\n",
    "heatmap_data = df.set_index('類別')[metrics].T\n",
    "\n",
    "im = ax3.imshow(heatmap_data.values, cmap='RdYlBu_r', aspect='auto', vmin=1, vmax=5)\n",
    "ax3.set_xticks(range(len(heatmap_data.columns)))\n",
    "ax3.set_xticklabels(heatmap_data.columns, rotation=45, ha='right', fontsize=8)\n",
    "ax3.set_yticks(range(len(heatmap_data.index)))\n",
    "ax3.set_yticklabels(metrics_en, fontsize=10)\n",
    "ax3.set_title('All Product Categories Rating Heatmap', fontsize=14, weight='bold')\n",
    "\n",
    "# 添加數值標籤\n",
    "for i in range(len(heatmap_data.index)):\n",
    "    for j in range(len(heatmap_data.columns)):\n",
    "        ax3.text(j, i, int(heatmap_data.iloc[i, j]), ha='center', va='center', \n",
    "                color='white' if heatmap_data.iloc[i, j] < 3 else 'black', fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax3, shrink=0.6)\n",
    "\n",
    "# 4. Bar Chart - Classification count statistics\n",
    "category_counts = df['Category_EN'].value_counts()\n",
    "bars = ax4.bar(category_counts.index, category_counts.values, \n",
    "               color=[category_colors[cat] for cat in category_counts.index])\n",
    "ax4.set_title('Product Classification Count Statistics', fontsize=14, weight='bold')\n",
    "ax4.set_ylabel('Number of Products', fontsize=12)\n",
    "\n",
    "# 在長條上添加數值\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.suptitle('Amazon Product Categories - Five Rating Indicators Visualization', fontsize=16, weight='bold', y=0.98)\n",
    "plt.show()\n",
    "\n",
    "# Print classification summary\n",
    "print(\"=== Amazon Product Classification Summary ===\")\n",
    "print(f\"Search Goods: {len(df[df['最終分類']=='搜尋品'])} categories\")\n",
    "print(f\"Experience Goods: {len(df[df['最終分類']=='經驗品'])} categories\") \n",
    "print(f\"Hybrid: {len(df[df['最終分類']=='混合型'])} categories\")\n",
    "print(\"\\nAverage indicators by classification:\")\n",
    "print(avg_scores.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三階段模型\n",
    "第一階段：資料預處理 - 建立基本變數、對數轉換、虛擬變數等\n",
    "第二階段：正交化處理 - 對Arousal和Valence進行正交化，避免多重共線性\n",
    "第三階段：建立交互項 - 創建平方項和交互項\n",
    "第四階段：準備三個模型 - 參考模型、基礎模型、完整模型\n",
    "第五階段：執行回歸分析 - 使用statsmodels進行OLS回歸\n",
    "第六階段：統計檢驗 - 嵌套模型F檢驗\n",
    "第七階段：結論分析 - 動態判斷最佳模型並提供建議"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Amazon評論有用性分析 - 完整回歸建模流程\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# =============================================\n",
    "# 第一部分：資料預處理和變數建立\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n第一階段：資料預處理...\")\n",
    "\n",
    "# 處理評論數據並計算基本屬性\n",
    "processed_df = df.with_columns([\n",
    "    # 1. 計算 title_processed 和 text_processed 的總長度\n",
    "    (pl.col(\"title_processed\").str.len_chars() + pl.col(\"text_processed\").str.len_chars()).alias(\"total_length\"),\n",
    "    \n",
    "    # 2. 計算評論發布距今天數（自然對數）\n",
    "    # unix timestamp 轉換為天數差異再取對數\n",
    "    ((pl.lit(1750089600000) - pl.col(\"timestamp\")) / 86400000).log().alias(\"longevity\"),\n",
    "    \n",
    "    # 3. 將rating轉換為float64\n",
    "    pl.col(\"rating\").cast(pl.Float64).alias(\"rating_float\"),\n",
    "    \n",
    "    # 4. 計算rating_number的自然對數（商品評論數量）\n",
    "    pl.col(\"rating_number\").log().alias(\"biz_review_counts_ln\")\n",
    "]).with_columns([\n",
    "    # 5. 計算每個 user_id 的評論總數\n",
    "    pl.col(\"user_id\").count().over(\"user_id\").alias(\"reviewer_posts\")\n",
    "]).with_columns([\n",
    "    # 6. 計算各種對數變換\n",
    "    pl.col(\"total_length\").log().alias(\"length_ln\"),\n",
    "    pl.col(\"reviewer_posts\").log().alias(\"reviewer_posts_ln\"),\n",
    "    \n",
    "    # 7. 創建rating的dummy變數（以rating=5為參考組）\n",
    "    pl.when(pl.col(\"rating_float\") == 1.0).then(1).otherwise(0).alias(\"rating_1\"),\n",
    "    pl.when(pl.col(\"rating_float\") == 2.0).then(1).otherwise(0).alias(\"rating_2\"),\n",
    "    pl.when(pl.col(\"rating_float\") == 3.0).then(1).otherwise(0).alias(\"rating_3\"),\n",
    "    pl.when(pl.col(\"rating_float\") == 4.0).then(1).otherwise(0).alias(\"rating_4\")\n",
    "    # rating_5 作為參考組，不需要創建\n",
    "])\n",
    "\n",
    "# 檢查helpful_vote的分布並決定是否取對數\n",
    "helpful_vote_stats = processed_df.select([\n",
    "    pl.col(\"helpful_vote\").mean().alias(\"mean\"),\n",
    "    pl.col(\"helpful_vote\").median().alias(\"median\"),\n",
    "    pl.col(\"helpful_vote\").std().alias(\"std\"),\n",
    "    pl.col(\"helpful_vote\").skew().alias(\"skewness\")\n",
    "])\n",
    "\n",
    "print(\"Helpful_vote 分布統計：\")\n",
    "print(helpful_vote_stats)\n",
    "\n",
    "# 如果偏度 > 1，表示右偏，需要取對數\n",
    "skewness_value = helpful_vote_stats.select(\"skewness\").item()\n",
    "if skewness_value > 1:\n",
    "    print(f\"helpful_vote 右偏 (偏度: {skewness_value:.3f})，將進行對數轉換\")\n",
    "    processed_df = processed_df.with_columns([\n",
    "        (pl.col(\"helpful_vote\") + 1).log().alias(\"helpful_vote_ln\")  # 加1避免log(0)\n",
    "    ])\n",
    "    dependent_var = \"helpful_vote_ln\"\n",
    "else:\n",
    "    print(f\"helpful_vote 分布正常 (偏度: {skewness_value:.3f})，不需要對數轉換\")\n",
    "    dependent_var = \"helpful_vote\"\n",
    "\n",
    "print(f\"依變數確定為: {dependent_var}\")\n",
    "\n",
    "# 正交化處理 Arousal 和 Valence\n",
    "print(\"\\n執行Arousal和Valence正交化處理...\")\n",
    "\n",
    "# 將 polars DataFrame 轉換為 numpy array 進行正交化\n",
    "arousal_vals = processed_df.select(\"Arousal\").to_numpy().flatten()\n",
    "valence_vals = processed_df.select(\"Valence\").to_numpy().flatten()\n",
    "\n",
    "# 移除缺失值的索引\n",
    "valid_indices = ~(np.isnan(arousal_vals) | np.isnan(valence_vals))\n",
    "arousal_clean = arousal_vals[valid_indices]\n",
    "valence_clean = valence_vals[valid_indices]\n",
    "\n",
    "# 對 Arousal 進行 Valence 的回歸，取殘差作為正交化的 Arousal\n",
    "reg_arousal = LinearRegression()\n",
    "reg_arousal.fit(valence_clean.reshape(-1, 1), arousal_clean)\n",
    "arousal_residuals = arousal_clean - reg_arousal.predict(valence_clean.reshape(-1, 1))\n",
    "\n",
    "# 對 Valence 進行 Arousal 的回歸，取殘差作為正交化的 Valence\n",
    "reg_valence = LinearRegression()\n",
    "reg_valence.fit(arousal_clean.reshape(-1, 1), valence_clean)\n",
    "valence_residuals = valence_clean - reg_valence.predict(arousal_clean.reshape(-1, 1))\n",
    "\n",
    "# 創建正交化結果的完整數組（包含原始的NaN位置）\n",
    "oArousal = np.full(len(arousal_vals), np.nan)\n",
    "oValence = np.full(len(valence_vals), np.nan)\n",
    "oArousal[valid_indices] = arousal_residuals\n",
    "oValence[valid_indices] = valence_residuals\n",
    "\n",
    "# 將正交化結果添加到DataFrame\n",
    "processed_df = processed_df.with_columns([\n",
    "    pl.Series(\"oArousal\", oArousal),\n",
    "    pl.Series(\"oValence\", oValence)\n",
    "])\n",
    "\n",
    "print(\"正交化完成！\")\n",
    "\n",
    "# =============================================\n",
    "# 第二部分：建立交互項和平方項\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n第二階段：建立交互項和平方項...\")\n",
    "\n",
    "# 建立交互項和平方項\n",
    "processed_df = processed_df.with_columns([\n",
    "    # oArousal的平方項\n",
    "    (pl.col(\"oArousal\") ** 2).alias(\"oArousal_squared\"),\n",
    "    \n",
    "    # oArousal和oValence的交互項\n",
    "    (pl.col(\"oArousal\") * pl.col(\"oValence\")).alias(\"oArousal_oValence_interaction\")\n",
    "])\n",
    "\n",
    "# =============================================\n",
    "# 第三部分：準備三個階層回歸模型\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n第三階段：準備三個階層回歸模型...\")\n",
    "\n",
    "# 準備各模型所需變數\n",
    "base_controls = [\n",
    "    \"length_ln\",           # β1: log(Length)\n",
    "    \"breadth\",             # β2: Breadth\n",
    "    \"readability\",         # β3: Readability\n",
    "    \"reviewer_posts_ln\",   # β4: log(ReviewerPosts)\n",
    "    \"longevity\",           # β5: log(Longevity)\n",
    "    \"rating_1\",            # β6: ReviewRating dummy\n",
    "    \"rating_2\",            # β7: ReviewRating dummy\n",
    "    \"rating_3\",            # β8: ReviewRating dummy\n",
    "    \"rating_4\",            # β9: ReviewRating dummy\n",
    "    \"biz_review_counts_ln\" # β10: log(BizReviewCounts)\n",
    "]\n",
    "\n",
    "# Model 1: 參考模型（不包含情感變數）\n",
    "model1_vars = [dependent_var] + base_controls\n",
    "\n",
    "# Model 2: 基礎模型（包含線性情感效應）\n",
    "model2_vars = [dependent_var] + [\n",
    "    \"oValence\",            # γ1: oValence\n",
    "    \"oArousal\"             # γ2: oArousal\n",
    "] + base_controls\n",
    "\n",
    "# Model 3: 完整模型（包含非線性和交互效應）\n",
    "model3_vars = [dependent_var] + [\n",
    "    \"oValence\",                        # γ1: oValence\n",
    "    \"oArousal\",                        # γ2: oArousal\n",
    "    \"oArousal_squared\",                # γ3: oArousal^2\n",
    "    \"oArousal_oValence_interaction\"    # γ4: oArousal*oValence\n",
    "] + base_controls\n",
    "\n",
    "# 準備各模型的資料集\n",
    "df_model1 = processed_df.select(model1_vars).drop_nulls()\n",
    "df_model2 = processed_df.select(model2_vars).drop_nulls()\n",
    "df_model3 = processed_df.select(model3_vars).drop_nulls()\n",
    "\n",
    "print(f\"Model 1 觀測數: {df_model1.shape[0]}\")\n",
    "print(f\"Model 2 觀測數: {df_model2.shape[0]}\")\n",
    "print(f\"Model 3 觀測數: {df_model3.shape[0]}\")\n",
    "\n",
    "# =============================================\n",
    "# 第四部分：執行回歸分析\n",
    "# =============================================\n",
    "\n",
    "# 轉換為pandas進行回歸分析\n",
    "def run_regression(df, model_name, dependent_var):\n",
    "    \"\"\"執行OLS回歸並返回結果\"\"\"\n",
    "    df_pd = df.to_pandas()\n",
    "    \n",
    "    # 分離依變數和自變數\n",
    "    y = df_pd[dependent_var]\n",
    "    X = df_pd.drop(columns=[dependent_var])\n",
    "    \n",
    "    # 加入常數項\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # 執行回歸\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    return model, X.columns.tolist()\n",
    "\n",
    "# 執行三個模型\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"第四階段：執行回歸分析...\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "model1_result, model1_vars_final = run_regression(df_model1, \"Model 1\", dependent_var)\n",
    "model2_result, model2_vars_final = run_regression(df_model2, \"Model 2\", dependent_var)\n",
    "model3_result, model3_vars_final = run_regression(df_model3, \"Model 3\", dependent_var)\n",
    "\n",
    "# =============================================\n",
    "# 第五部分：結果呈現和分析\n",
    "# =============================================\n",
    "\n",
    "# 函數：格式化回歸結果摘要\n",
    "def print_model_summary(model, model_name, equation):\n",
    "    \"\"\"列印模型摘要\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"方程式: {equation}\")\n",
    "    print(f\"\\n觀測數: {int(model.nobs)}\")\n",
    "    print(f\"R-squared: {model.rsquared:.4f}\")\n",
    "    print(f\"Adjusted R-squared: {model.rsquared_adj:.4f}\")\n",
    "    print(f\"F-statistic: {model.fvalue:.4f} (p-value: {model.f_pvalue:.4e})\")\n",
    "    print(f\"AIC: {model.aic:.4f}\")\n",
    "    print(f\"BIC: {model.bic:.4f}\")\n",
    "    \n",
    "    # 顯示關鍵係數\n",
    "    print(f\"\\n關鍵係數:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'變數':<25} {'係數':<10} {'標準誤':<10} {'t值':<8} {'p值':<10} {'顯著性':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for var in model.params.index:\n",
    "        if var != \"const\":\n",
    "            coef = model.params[var]\n",
    "            se = model.bse[var]\n",
    "            t_val = model.tvalues[var]\n",
    "            p_val = model.pvalues[var]\n",
    "            sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
    "            print(f\"{var:<25} {coef:<10.4f} {se:<10.4f} {t_val:<8.3f} {p_val:<10.4f} {sig:<8}\")\n",
    "\n",
    "# 列印各模型結果\n",
    "equations = {\n",
    "    \"Model 1\": \"log(Helpfulness) = α + β1*log(Length) + β2*Breadth + β3*Readability + β4*log(ReviewerPosts) + β5*log(Longevity) + β6~9*ReviewRating + β10*log(BizReviewCounts) + ε\",\n",
    "    \"Model 2\": \"log(Helpfulness) = α + γ1*oValence + γ2*oArousal + β1*log(Length) + β2*Breadth + β3*Readability + β4*log(ReviewerPosts) + β5*log(Longevity) + β6~9*ReviewRating + β10*log(BizReviewCounts) + ε\",\n",
    "    \"Model 3\": \"log(Helpfulness) = α + γ1*oValence + γ2*oArousal + γ3*oArousal² + γ4*oArousal*oValence + β1*log(Length) + β2*Breadth + β3*Readability + β4*log(ReviewerPosts) + β5*log(Longevity) + β6~9*ReviewRating + β10*log(BizReviewCounts) + ε\"\n",
    "}\n",
    "\n",
    "print_model_summary(model1_result, \"Model 1 (參考模型)\", equations[\"Model 1\"])\n",
    "print_model_summary(model2_result, \"Model 2 (基礎模型)\", equations[\"Model 2\"])\n",
    "print_model_summary(model3_result, \"Model 3 (完整模型)\", equations[\"Model 3\"])\n",
    "\n",
    "# 模型比較表\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"模型比較摘要\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "comparison_data = {\n",
    "    \"模型\": [\"Model 1 (參考)\", \"Model 2 (基礎)\", \"Model 3 (完整)\"],\n",
    "    \"觀測數\": [int(model1_result.nobs), int(model2_result.nobs), int(model3_result.nobs)],\n",
    "    \"R²\": [f\"{model1_result.rsquared:.4f}\", f\"{model2_result.rsquared:.4f}\", f\"{model3_result.rsquared:.4f}\"],\n",
    "    \"Adj R²\": [f\"{model1_result.rsquared_adj:.4f}\", f\"{model2_result.rsquared_adj:.4f}\", f\"{model3_result.rsquared_adj:.4f}\"],\n",
    "    \"AIC\": [f\"{model1_result.aic:.2f}\", f\"{model2_result.aic:.2f}\", f\"{model3_result.aic:.2f}\"],\n",
    "    \"BIC\": [f\"{model1_result.bic:.2f}\", f\"{model2_result.bic:.2f}\", f\"{model3_result.bic:.2f}\"],\n",
    "    \"F統計量\": [f\"{model1_result.fvalue:.2f}\", f\"{model2_result.fvalue:.2f}\", f\"{model3_result.fvalue:.2f}\"]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# =============================================\n",
    "# 第六部分：統計檢驗\n",
    "# =============================================\n",
    "\n",
    "# 進行嵌套模型F檢驗\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"嵌套模型F檢驗\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "def nested_f_test(restricted_model, full_model, model1_name, model2_name):\n",
    "    \"\"\"執行嵌套模型F檢驗\"\"\"\n",
    "    rss_restricted = restricted_model.ssr\n",
    "    rss_full = full_model.ssr\n",
    "    df_restricted = restricted_model.df_resid\n",
    "    df_full = full_model.df_resid\n",
    "    \n",
    "    # F統計量\n",
    "    f_stat = ((rss_restricted - rss_full) / (df_restricted - df_full)) / (rss_full / df_full)\n",
    "    p_value = 1 - stats.f.cdf(f_stat, df_restricted - df_full, df_full)\n",
    "    \n",
    "    print(f\"\\n{model1_name} vs {model2_name}:\")\n",
    "    print(f\"F統計量: {f_stat:.4f}\")\n",
    "    print(f\"p值: {p_value:.4e}\")\n",
    "    print(f\"自由度: ({df_restricted - df_full}, {df_full})\")\n",
    "    significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"不顯著\"\n",
    "    print(f\"結果: {significance}\")\n",
    "    \n",
    "    return f_stat, p_value\n",
    "\n",
    "# 執行F檢驗\n",
    "f1, p1 = nested_f_test(model1_result, model2_result, \"Model 1\", \"Model 2\")\n",
    "f2, p2 = nested_f_test(model2_result, model3_result, \"Model 2\", \"Model 3\")\n",
    "f3, p3 = nested_f_test(model1_result, model3_result, \"Model 1\", \"Model 3\")\n",
    "\n",
    "# =============================================\n",
    "# 第七部分：研究結論\n",
    "# =============================================\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"研究結論\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# 動態判斷最佳模型\n",
    "models_results = {\n",
    "    \"Model 1\": model1_result,\n",
    "    \"Model 2\": model2_result, \n",
    "    \"Model 3\": model3_result\n",
    "}\n",
    "\n",
    "# 找出最高Adjusted R²的模型\n",
    "best_r2_model = max(models_results.keys(), key=lambda x: models_results[x].rsquared_adj)\n",
    "best_r2_value = models_results[best_r2_model].rsquared_adj\n",
    "\n",
    "# 找出最低AIC的模型\n",
    "best_aic_model = min(models_results.keys(), key=lambda x: models_results[x].aic)\n",
    "best_aic_value = models_results[best_aic_model].aic\n",
    "\n",
    "print(\"1. 模型比較結果:\")\n",
    "print(f\"   - {best_r2_model}具有最高的解釋力 (Adj R² = {best_r2_value:.4f})\")\n",
    "print(f\"   - {best_aic_model}具有最低的AIC值 ({best_aic_value:.2f})，表示最佳的模型適配\")\n",
    "\n",
    "# 檢查AIC和Adj R²是否指向同一個模型\n",
    "if best_r2_model == best_aic_model:\n",
    "    print(f\"   - 基於Adj R²和AIC準則，{best_aic_model}表現最佳\")\n",
    "else:\n",
    "    print(f\"   - 注意：Adj R²和AIC準則指向不同模型，需進一步考慮研究目的選擇\")\n",
    "\n",
    "print(\"\\n2. 嵌套檢驗結果:\")\n",
    "if p1 < 0.05:\n",
    "    print(\"   - 加入線性情感效應顯著改善模型 (Model 2 > Model 1)\")\n",
    "else:\n",
    "    print(\"   - 加入線性情感效應未顯著改善模型\")\n",
    "\n",
    "if p2 < 0.05:\n",
    "    print(\"   - 加入非線性和交互效應顯著改善模型 (Model 3 > Model 2)\")\n",
    "else:\n",
    "    print(\"   - 加入非線性和交互效應未顯著改善模型\")\n",
    "\n",
    "print(\"\\n3. 關鍵發現:\")\n",
    "if 'oArousal_squared' in model3_result.params.index:\n",
    "    arousal_sq_coef = model3_result.params['oArousal_squared']\n",
    "    arousal_sq_pval = model3_result.pvalues['oArousal_squared']\n",
    "    if arousal_sq_pval < 0.05:\n",
    "        print(f\"   - 情感喚醒度存在顯著的非線性效應 (γ3 = {arousal_sq_coef:.4f}, p < 0.05)\")\n",
    "    else:   \n",
    "        print(\"   - 情感喚醒度的非線性效應不顯著\")\n",
    "\n",
    "if 'oArousal_oValence_interaction' in model3_result.params.index:\n",
    "    interaction_coef = model3_result.params['oArousal_oValence_interaction']\n",
    "    interaction_pval = model3_result.pvalues['oArousal_oValence_interaction']\n",
    "    if interaction_pval < 0.05:\n",
    "        print(f\"   - 情感效價具有顯著的調節效應 (γ4 = {interaction_coef:.4f}, p < 0.05)\")\n",
    "    else:\n",
    "        print(\"   - 情感效價的調節效應不顯著\")\n",
    "\n",
    "print(f\"\\n4. 最終建議:\")\n",
    "if best_r2_model == best_aic_model:\n",
    "    print(f\"   基於Adj R²和AIC準則的一致性，推薦使用{best_aic_model}作為最終分析模型。\")\n",
    "else:\n",
    "    print(f\"   Adj R²支持{best_r2_model}，AIC支持{best_aic_model}。\")\n",
    "    print(f\"   建議根據研究目的選擇：\")\n",
    "    print(f\"   - 若重視解釋力最大化：選擇{best_r2_model}\")\n",
    "    print(f\"   - 若重視模型簡約性：選擇{best_aic_model}\")\n",
    "    print(f\"   - 若理論假設重要：評估F檢驗結果決定是否需要複雜模型\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"分析完成！\")\n",
    "print(f\"{'='*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 倒U型關係驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*120)\n",
    "print(\"倒U型關係驗證 - 遵循 Haans et al. (2016) 方法\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "def validate_inverted_u_relationship(model_result, data_df):\n",
    "    \"\"\"\n",
    "    驗證倒U型關係的完整程序\n",
    "    參數:\n",
    "    - model_result: 完整模型(Model 3)的回歸結果\n",
    "    - data_df: 包含原始數據的DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # 步驟1: 確認二次項係數顯著且為負\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"步驟1: 驗證二次項係數 (γ3) 顯著且為負\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 提取關鍵係數\n",
    "    gamma2 = model_result.params['oArousal']  # oArousal線性項\n",
    "    gamma3 = model_result.params['oArousal_squared']  # oArousal平方項\n",
    "    gamma4 = model_result.params['oArousal_oValence_interaction']  # 交互項\n",
    "    \n",
    "    # 提取p值\n",
    "    p_gamma2 = model_result.pvalues['oArousal']\n",
    "    p_gamma3 = model_result.pvalues['oArousal_squared']\n",
    "    p_gamma4 = model_result.pvalues['oArousal_oValence_interaction']\n",
    "    \n",
    "    print(f\"γ2 (oArousal): {gamma2:.6f} (p = {p_gamma2:.6f})\")\n",
    "    print(f\"γ3 (oArousal²): {gamma3:.6f} (p = {p_gamma3:.6f})\")\n",
    "    print(f\"γ4 (oArousal×oValence): {gamma4:.6f} (p = {p_gamma4:.6f})\")\n",
    "    \n",
    "    # 判斷二次項係數\n",
    "    if gamma3 < 0 and p_gamma3 < 0.05:\n",
    "        print(f\"\\n✓ 驗證通過: γ3 = {gamma3:.6f} < 0 且顯著 (p < 0.05)\")\n",
    "        print(\"  → 支持倒U型關係假設\")\n",
    "        relationship_type = \"倒U型\"\n",
    "    elif gamma3 > 0 and p_gamma3 < 0.05:\n",
    "        print(f\"\\n✗ γ3 = {gamma3:.6f} > 0 且顯著\")\n",
    "        print(\"  → 支持U型關係，非倒U型\")\n",
    "        relationship_type = \"U型\"\n",
    "    else:\n",
    "        print(f\"\\n✗ γ3 = {gamma3:.6f} 不顯著 (p = {p_gamma3:.6f})\")\n",
    "        print(\"  → 無顯著二次關係\")\n",
    "        relationship_type = \"無二次關係\"\n",
    "    \n",
    "    # 步驟2: 測試斜率在低點和高點的顯著性\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"步驟2: 測試斜率在不同oArousal水平的顯著性\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 計算oArousal和oValence的分位數\n",
    "    arousal_data = data_df['oArousal']\n",
    "    valence_data = data_df['oValence']\n",
    "    \n",
    "    # 定義低、中、高水平\n",
    "    arousal_low = np.percentile(arousal_data, 10)   # 10th percentile\n",
    "    arousal_high = np.percentile(arousal_data, 90)  # 90th percentile\n",
    "    \n",
    "    valence_low = np.percentile(valence_data, 10)\n",
    "    valence_med = np.percentile(valence_data, 50)\n",
    "    valence_high = np.percentile(valence_data, 90)\n",
    "    \n",
    "    print(f\"oArousal範圍: 低點 = {arousal_low:.3f}, 高點 = {arousal_high:.3f}\")\n",
    "    print(f\"oValence水平: 低 = {valence_low:.3f}, 中 = {valence_med:.3f}, 高 = {valence_high:.3f}\")\n",
    "    \n",
    "    # 斜率計算公式: 斜率 = γ2 + 2γ3*oArousal + γ4*oValence\n",
    "    print(f\"\\n斜率公式: 斜率 = {gamma2:.6f} + 2×{gamma3:.6f}×oArousal + {gamma4:.6f}×oValence\")\n",
    "    \n",
    "    # 計算不同情況下的斜率\n",
    "    valence_levels = [\n",
    "        (\"低效價\", valence_low),\n",
    "        (\"中效價\", valence_med), \n",
    "        (\"高效價\", valence_high)\n",
    "    ]\n",
    "    \n",
    "    arousal_levels = [\n",
    "        (\"低喚醒\", arousal_low),\n",
    "        (\"高喚醒\", arousal_high)\n",
    "    ]\n",
    "    \n",
    "    slope_results = []\n",
    "    \n",
    "    print(f\"\\n{'情感效價':<10} {'喚醒度':<10} {'斜率':<12} {'標準誤':<10} {'t值':<8} {'p值':<10} {'顯著性':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for val_name, val_level in valence_levels:\n",
    "        for aro_name, aro_level in arousal_levels:\n",
    "            # 計算斜率\n",
    "            slope = gamma2 + 2 * gamma3 * aro_level + gamma4 * val_level\n",
    "            \n",
    "            # 計算斜率的標準誤 (使用Delta方法)\n",
    "            # Var(slope) = Var(γ2) + 4*aro²*Var(γ3) + val²*Var(γ4) + \n",
    "            #              4*aro*Cov(γ2,γ3) + 2*val*Cov(γ2,γ4) + 4*aro*val*Cov(γ3,γ4)\n",
    "            \n",
    "            var_gamma2 = model_result.cov_params().loc['oArousal', 'oArousal']\n",
    "            var_gamma3 = model_result.cov_params().loc['oArousal_squared', 'oArousal_squared']\n",
    "            var_gamma4 = model_result.cov_params().loc['oArousal_oValence_interaction', 'oArousal_oValence_interaction']\n",
    "            \n",
    "            cov_gamma2_gamma3 = model_result.cov_params().loc['oArousal', 'oArousal_squared']\n",
    "            cov_gamma2_gamma4 = model_result.cov_params().loc['oArousal', 'oArousal_oValence_interaction']\n",
    "            cov_gamma3_gamma4 = model_result.cov_params().loc['oArousal_squared', 'oArousal_oValence_interaction']\n",
    "            \n",
    "            slope_var = (var_gamma2 + \n",
    "                        4 * aro_level**2 * var_gamma3 + \n",
    "                        val_level**2 * var_gamma4 + \n",
    "                        4 * aro_level * cov_gamma2_gamma3 + \n",
    "                        2 * val_level * cov_gamma2_gamma4 + \n",
    "                        4 * aro_level * val_level * cov_gamma3_gamma4)\n",
    "            \n",
    "            slope_se = np.sqrt(slope_var)\n",
    "            t_stat = slope / slope_se\n",
    "            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), model_result.df_resid))\n",
    "            \n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
    "            \n",
    "            print(f\"{val_name:<10} {aro_name:<10} {slope:<12.6f} {slope_se:<10.6f} {t_stat:<8.3f} {p_value:<10.6f} {significance:<8}\")\n",
    "            \n",
    "            slope_results.append({\n",
    "                'valence_level': val_name,\n",
    "                'arousal_level': aro_name,\n",
    "                'slope': slope,\n",
    "                'se': slope_se,\n",
    "                't_stat': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05\n",
    "            })\n",
    "    \n",
    "    # 步驟3: 計算並驗證轉折點\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"步驟3: 計算並驗證轉折點位置\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"轉折點公式: oArousal* = (-γ2 - γ4×oValence) / (2×γ3)\")\n",
    "    print(f\"代入係數: oArousal* = (-{gamma2:.6f} - {gamma4:.6f}×oValence) / (2×{gamma3:.6f})\")\n",
    "    \n",
    "    turning_points = []\n",
    "    \n",
    "    print(f\"\\n{'情感效價水平':<15} {'轉折點':<12} {'標準誤':<10} {'t值':<8} {'p值':<10} {'顯著性':<8} {'在範圍內':<10}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for val_name, val_level in valence_levels:\n",
    "        # 計算轉折點\n",
    "        turning_point = (-gamma2 - gamma4 * val_level) / (2 * gamma3)\n",
    "        \n",
    "        # 計算轉折點的標準誤 (Delta方法)\n",
    "        # 對於 f(γ2,γ3,γ4) = (-γ2 - γ4*val) / (2*γ3)\n",
    "        # 偏導數:\n",
    "        df_dgamma2 = -1 / (2 * gamma3)\n",
    "        df_dgamma3 = (gamma2 + gamma4 * val_level) / (2 * gamma3**2)\n",
    "        df_dgamma4 = -val_level / (2 * gamma3)\n",
    "        \n",
    "        # 計算方差\n",
    "        tp_var = (df_dgamma2**2 * var_gamma2 + \n",
    "                 df_dgamma3**2 * var_gamma3 + \n",
    "                 df_dgamma4**2 * var_gamma4 + \n",
    "                 2 * df_dgamma2 * df_dgamma3 * cov_gamma2_gamma3 + \n",
    "                 2 * df_dgamma2 * df_dgamma4 * cov_gamma2_gamma4 + \n",
    "                 2 * df_dgamma3 * df_dgamma4 * cov_gamma3_gamma4)\n",
    "        \n",
    "        tp_se = np.sqrt(tp_var)\n",
    "        tp_t_stat = turning_point / tp_se\n",
    "        tp_p_value = 2 * (1 - stats.t.cdf(abs(tp_t_stat), model_result.df_resid))\n",
    "        \n",
    "        # 檢查是否在數據範圍內\n",
    "        arousal_min = arousal_data.min()\n",
    "        arousal_max = arousal_data.max()\n",
    "        in_range = arousal_min <= turning_point <= arousal_max\n",
    "        \n",
    "        significance = \"***\" if tp_p_value < 0.001 else \"**\" if tp_p_value < 0.01 else \"*\" if tp_p_value < 0.05 else \"\"\n",
    "        range_status = \"是\" if in_range else \"否\"\n",
    "        \n",
    "        print(f\"{val_name:<15} {turning_point:<12.3f} {tp_se:<10.6f} {tp_t_stat:<8.3f} {tp_p_value:<10.6f} {significance:<8} {range_status:<10}\")\n",
    "        \n",
    "        turning_points.append({\n",
    "            'valence_level': val_name,\n",
    "            'turning_point': turning_point,\n",
    "            'se': tp_se,\n",
    "            't_stat': tp_t_stat,\n",
    "            'p_value': tp_p_value,\n",
    "            'significant': tp_p_value < 0.05,\n",
    "            'in_range': in_range\n",
    "        })\n",
    "    \n",
    "    print(f\"\\noArousal數據範圍: [{arousal_min:.3f}, {arousal_max:.3f}]\")\n",
    "    \n",
    "    # 總結驗證結果\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"驗證結果總結\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 檢查斜率模式\n",
    "    low_slopes_positive = all([r['slope'] > 0 and r['significant'] for r in slope_results if r['arousal_level'] == '低喚醒'])\n",
    "    high_slopes_negative = all([r['slope'] < 0 and r['significant'] for r in slope_results if r['arousal_level'] == '高喚醒'])\n",
    "    \n",
    "    all_turning_points_significant = all([tp['significant'] for tp in turning_points])\n",
    "    all_turning_points_in_range = all([tp['in_range'] for tp in turning_points])\n",
    "    \n",
    "    print(\"1. 二次項係數檢驗:\")\n",
    "    if gamma3 < 0 and p_gamma3 < 0.05:\n",
    "        print(\"   ✓ γ3顯著為負，支持倒U型關係\")\n",
    "    else:\n",
    "        print(\"   ✗ γ3不符合倒U型關係要求\")\n",
    "    \n",
    "    print(\"\\n2. 斜率檢驗:\")\n",
    "    if low_slopes_positive:\n",
    "        print(\"   ✓ 低喚醒度時斜率均為正且顯著\")\n",
    "    else:\n",
    "        print(\"   ✗ 低喚醒度時斜率不符合預期\")\n",
    "        \n",
    "    if high_slopes_negative:\n",
    "        print(\"   ✓ 高喚醒度時斜率均為負且顯著\")\n",
    "    else:\n",
    "        print(\"   ✗ 高喚醒度時斜率不符合預期\")\n",
    "    \n",
    "    print(\"\\n3. 轉折點檢驗:\")\n",
    "    if all_turning_points_significant:\n",
    "        print(\"   ✓ 所有轉折點均顯著\")\n",
    "    else:\n",
    "        print(\"   ✗ 部分轉折點不顯著\")\n",
    "        \n",
    "    if all_turning_points_in_range:\n",
    "        print(\"   ✓ 所有轉折點均在數據範圍內\")\n",
    "    else:\n",
    "        print(\"   ✗ 部分轉折點超出數據範圍\")\n",
    "    \n",
    "    # 最終結論\n",
    "    validation_passed = (gamma3 < 0 and p_gamma3 < 0.05 and \n",
    "                        low_slopes_positive and high_slopes_negative and\n",
    "                        all_turning_points_significant and all_turning_points_in_range)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    if validation_passed:\n",
    "        print(\"🎉 驗證結論: 倒U型關係得到完全驗證！\")\n",
    "        print(\"   所有三個驗證步驟均通過，支持以下假設:\")\n",
    "        print(\"   - 情感喚醒度與評論有用性存在倒U型關係\")\n",
    "        print(\"   - 情感效價對此倒U型關係具有調節作用\")\n",
    "    else:\n",
    "        print(\"⚠️  驗證結論: 倒U型關係未完全得到驗證\")\n",
    "        print(\"   部分驗證步驟未通過，需要謹慎解釋結果\")\n",
    "    \n",
    "    return {\n",
    "        'coefficients': {'gamma2': gamma2, 'gamma3': gamma3, 'gamma4': gamma4},\n",
    "        'slope_results': slope_results,\n",
    "        'turning_points': turning_points,\n",
    "        'validation_passed': validation_passed\n",
    "    }\n",
    "\n",
    "# 使用範例 (假設model3_result和final_df已經存在)\n",
    "validation_results = validate_inverted_u_relationship(model3_result, final_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"使用說明:\")\n",
    "print(\"請確保已運行前面的回歸分析，然後執行:\")\n",
    "print(\"validation_results = validate_inverted_u_relationship(model3_result, final_df)\")\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四階層模型(for H2)，因倒U無法驗證因此無用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Amazon評論有用性分析 - 完整回歸建模流程\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# =============================================\n",
    "# 第一部分：資料預處理和變數建立\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n第一階段：資料預處理...\")\n",
    "\n",
    "# 處理評論數據並計算基本屬性\n",
    "processed_df = experience_df.with_columns([\n",
    "    # 1. 計算 title_processed 和 text_processed 的總長度\n",
    "    (pl.col(\"title_processed\").str.len_chars() + pl.col(\"text_processed\").str.len_chars()).alias(\"total_length\"),\n",
    "    \n",
    "    # 2. 計算評論發布距今天數（自然對數）\n",
    "    # unix timestamp 轉換為天數差異再取對數\n",
    "    ((pl.lit(1750089600000) - pl.col(\"timestamp\")) / 86400000).log().alias(\"longevity\"),\n",
    "    \n",
    "    # 3. 將rating轉換為float64\n",
    "    pl.col(\"rating\").cast(pl.Float64).alias(\"rating_float\"),\n",
    "    \n",
    "    # 4. 計算rating_number的自然對數（商品評論數量）\n",
    "    pl.col(\"rating_number\").log().alias(\"biz_review_counts_ln\")\n",
    "]).with_columns([\n",
    "    # 5. 計算每個 user_id 的評論總數\n",
    "    pl.col(\"user_id\").count().over(\"user_id\").alias(\"reviewer_posts\")\n",
    "]).with_columns([\n",
    "    # 6. 計算各種對數變換\n",
    "    pl.col(\"total_length\").log().alias(\"length_ln\"),\n",
    "    pl.col(\"reviewer_posts\").log().alias(\"reviewer_posts_ln\"),\n",
    "    \n",
    "    # 7. 創建rating的dummy變數（以rating=5為參考組）\n",
    "    pl.when(pl.col(\"rating_float\") == 1.0).then(1).otherwise(0).alias(\"rating_1\"),\n",
    "    pl.when(pl.col(\"rating_float\") == 2.0).then(1).otherwise(0).alias(\"rating_2\"),\n",
    "    pl.when(pl.col(\"rating_float\") == 3.0).then(1).otherwise(0).alias(\"rating_3\"),\n",
    "    pl.when(pl.col(\"rating_float\") == 4.0).then(1).otherwise(0).alias(\"rating_4\")\n",
    "    # rating_5 作為參考組，不需要創建\n",
    "])\n",
    "\n",
    "# 檢查helpful_vote的分布並決定是否取對數\n",
    "helpful_vote_stats = processed_df.select([\n",
    "    pl.col(\"helpful_vote\").mean().alias(\"mean\"),\n",
    "    pl.col(\"helpful_vote\").median().alias(\"median\"),\n",
    "    pl.col(\"helpful_vote\").std().alias(\"std\"),\n",
    "    pl.col(\"helpful_vote\").skew().alias(\"skewness\")\n",
    "])\n",
    "\n",
    "print(\"Helpful_vote 分布統計：\")\n",
    "print(helpful_vote_stats)\n",
    "\n",
    "# 如果偏度 > 1，表示右偏，需要取對數\n",
    "skewness_value = helpful_vote_stats.select(\"skewness\").item()\n",
    "if skewness_value > 1:\n",
    "    print(f\"helpful_vote 右偏 (偏度: {skewness_value:.3f})，將進行對數轉換\")\n",
    "    processed_df = processed_df.with_columns([\n",
    "        (pl.col(\"helpful_vote\") + 1).log().alias(\"helpful_vote_ln\")  # 加1避免log(0)\n",
    "    ])\n",
    "    dependent_var = \"helpful_vote_ln\"\n",
    "else:\n",
    "    print(f\"helpful_vote 分布正常 (偏度: {skewness_value:.3f})，不需要對數轉換\")\n",
    "    dependent_var = \"helpful_vote\"\n",
    "\n",
    "print(f\"依變數確定為: {dependent_var}\")\n",
    "\n",
    "# 正交化處理 Arousal 和 Valence\n",
    "print(\"\\n執行Arousal和Valence正交化處理...\")\n",
    "\n",
    "# 將 polars DataFrame 轉換為 numpy array 進行正交化\n",
    "arousal_vals = processed_df.select(\"Arousal\").to_numpy().flatten()\n",
    "valence_vals = processed_df.select(\"Valence\").to_numpy().flatten()\n",
    "\n",
    "# 移除缺失值的索引\n",
    "valid_indices = ~(np.isnan(arousal_vals) | np.isnan(valence_vals))\n",
    "arousal_clean = arousal_vals[valid_indices]\n",
    "valence_clean = valence_vals[valid_indices]\n",
    "\n",
    "# 對 Arousal 進行 Valence 的回歸，取殘差作為正交化的 Arousal\n",
    "reg_arousal = LinearRegression()\n",
    "reg_arousal.fit(valence_clean.reshape(-1, 1), arousal_clean)\n",
    "arousal_residuals = arousal_clean - reg_arousal.predict(valence_clean.reshape(-1, 1))\n",
    "\n",
    "# 對 Valence 進行 Arousal 的回歸，取殘差作為正交化的 Valence\n",
    "reg_valence = LinearRegression()\n",
    "reg_valence.fit(arousal_clean.reshape(-1, 1), valence_clean)\n",
    "valence_residuals = valence_clean - reg_valence.predict(arousal_clean.reshape(-1, 1))\n",
    "\n",
    "# 創建正交化結果的完整數組（包含原始的NaN位置）\n",
    "oArousal = np.full(len(arousal_vals), np.nan)\n",
    "oValence = np.full(len(valence_vals), np.nan)\n",
    "oArousal[valid_indices] = arousal_residuals\n",
    "oValence[valid_indices] = valence_residuals\n",
    "\n",
    "# 將正交化結果添加到DataFrame\n",
    "processed_df = processed_df.with_columns([\n",
    "    pl.Series(\"oArousal\", oArousal),\n",
    "    pl.Series(\"oValence\", oValence)\n",
    "])\n",
    "\n",
    "print(\"正交化完成！\")\n",
    "\n",
    "# =============================================\n",
    "# 第二部分：建立交互項和平方項\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n第二階段：建立交互項和平方項...\")\n",
    "\n",
    "# 建立交互項和平方項\n",
    "processed_df = processed_df.with_columns([\n",
    "    # oArousal的平方項\n",
    "    (pl.col(\"oArousal\") ** 2).alias(\"oArousal_squared\"),\n",
    "    \n",
    "    # oArousal和oValence的交互項\n",
    "    (pl.col(\"oArousal\") * pl.col(\"oValence\")).alias(\"oArousal_oValence_interaction\"),\n",
    "    \n",
    "    # 新增：oValence和oArousal²的交互項 (γ5項)\n",
    "    (pl.col(\"oValence\") * (pl.col(\"oArousal\") ** 2)).alias(\"oValence_oArousal_squared_interaction\")\n",
    "])\n",
    "\n",
    "# =============================================\n",
    "# 第三部分：準備四個階層回歸模型\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n第三階段：準備四個階層回歸模型...\")\n",
    "\n",
    "# 準備各模型所需變數\n",
    "base_controls = [\n",
    "    \"length_ln\",           # β1: log(Length)\n",
    "    \"breadth\",             # β2: Breadth\n",
    "    \"readability\",         # β3: Readability\n",
    "    \"reviewer_posts_ln\",   # β4: log(ReviewerPosts)\n",
    "    \"longevity\",           # β5: log(Longevity)\n",
    "    \"rating_1\",            # β6: ReviewRating dummy\n",
    "    \"rating_2\",            # β7: ReviewRating dummy\n",
    "    \"rating_3\",            # β8: ReviewRating dummy\n",
    "    \"rating_4\",            # β9: ReviewRating dummy\n",
    "    \"biz_review_counts_ln\" # β10: log(BizReviewCounts)\n",
    "]\n",
    "\n",
    "# Model 1: 參考模型（不包含情感變數）\n",
    "model1_vars = [dependent_var] + base_controls\n",
    "\n",
    "# Model 2: 基礎模型（包含線性情感效應）\n",
    "model2_vars = [dependent_var] + [\n",
    "    \"oValence\",            # γ1: oValence\n",
    "    \"oArousal\"             # γ2: oArousal\n",
    "] + base_controls\n",
    "\n",
    "# Model 3: 包含非線性效應的模型\n",
    "model3_vars = [dependent_var] + [\n",
    "    \"oValence\",                        # γ1: oValence\n",
    "    \"oArousal\",                        # γ2: oArousal\n",
    "    \"oArousal_squared\",                # γ3: oArousal^2\n",
    "    \"oArousal_oValence_interaction\"    # γ4: oArousal*oValence\n",
    "] + base_controls\n",
    "\n",
    "# Model 4: 完整模型（包含所有交互項）- 方程式(4)\n",
    "model4_vars = [dependent_var] + [\n",
    "    \"oValence\",                                # γ1: oValence\n",
    "    \"oArousal\",                                # γ2: oArousal\n",
    "    \"oArousal_squared\",                        # γ3: oArousal^2\n",
    "    \"oArousal_oValence_interaction\",           # γ4: oArousal*oValence\n",
    "    \"oValence_oArousal_squared_interaction\"    # γ5: oValence*oArousal^2\n",
    "] + base_controls\n",
    "\n",
    "# 準備各模型的資料集\n",
    "df_model1 = processed_df.select(model1_vars).drop_nulls()\n",
    "df_model2 = processed_df.select(model2_vars).drop_nulls()\n",
    "df_model3 = processed_df.select(model3_vars).drop_nulls()\n",
    "df_model4 = processed_df.select(model4_vars).drop_nulls()\n",
    "\n",
    "print(f\"Model 1 觀測數: {df_model1.shape[0]}\")\n",
    "print(f\"Model 2 觀測數: {df_model2.shape[0]}\")\n",
    "print(f\"Model 3 觀測數: {df_model3.shape[0]}\")\n",
    "print(f\"Model 4 觀測數: {df_model4.shape[0]}\")\n",
    "\n",
    "# =============================================\n",
    "# 第四部分：執行回歸分析\n",
    "# =============================================\n",
    "\n",
    "# 轉換為pandas進行回歸分析\n",
    "def run_regression(df, model_name, dependent_var):\n",
    "    \"\"\"執行OLS回歸並返回結果\"\"\"\n",
    "    df_pd = df.to_pandas()\n",
    "    \n",
    "    # 分離依變數和自變數\n",
    "    y = df_pd[dependent_var]\n",
    "    X = df_pd.drop(columns=[dependent_var])\n",
    "    \n",
    "    # 加入常數項\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # 執行回歸\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    return model, X.columns.tolist()\n",
    "\n",
    "# 執行四個模型\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"第四階段：執行回歸分析...\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "model1_result, model1_vars_final = run_regression(df_model1, \"Model 1\", dependent_var)\n",
    "model2_result, model2_vars_final = run_regression(df_model2, \"Model 2\", dependent_var)\n",
    "model3_result, model3_vars_final = run_regression(df_model3, \"Model 3\", dependent_var)\n",
    "model4_result, model4_vars_final = run_regression(df_model4, \"Model 4\", dependent_var)\n",
    "\n",
    "# =============================================\n",
    "# 第五部分：結果呈現和分析\n",
    "# =============================================\n",
    "\n",
    "# 函數：格式化回歸結果摘要\n",
    "def print_model_summary(model, model_name, equation):\n",
    "    \"\"\"列印模型摘要\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"方程式: {equation}\")\n",
    "    print(f\"\\n觀測數: {int(model.nobs)}\")\n",
    "    print(f\"R-squared: {model.rsquared:.4f}\")\n",
    "    print(f\"Adjusted R-squared: {model.rsquared_adj:.4f}\")\n",
    "    print(f\"F-statistic: {model.fvalue:.4f} (p-value: {model.f_pvalue:.4e})\")\n",
    "    print(f\"AIC: {model.aic:.4f}\")\n",
    "    print(f\"BIC: {model.bic:.4f}\")\n",
    "    \n",
    "    # 顯示關鍵係數\n",
    "    print(f\"\\n關鍵係數:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'變數':<35} {'係數':<10} {'標準誤':<10} {'t值':<8} {'p值':<10} {'顯著性':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for var in model.params.index:\n",
    "        if var != \"const\":\n",
    "            coef = model.params[var]\n",
    "            se = model.bse[var]\n",
    "            t_val = model.tvalues[var]\n",
    "            p_val = model.pvalues[var]\n",
    "            sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
    "            print(f\"{var:<35} {coef:<10.4f} {se:<10.4f} {t_val:<8.3f} {p_val:<10.4f} {sig:<8}\")\n",
    "\n",
    "# 列印各模型結果\n",
    "equations = {\n",
    "    \"Model 1\": \"log(Helpfulness) = α + β1*log(Length) + β2*Breadth + β3*Readability + β4*log(ReviewerPosts) + β5*log(Longevity) + β6~9*ReviewRating + β10*log(BizReviewCounts) + ε\",\n",
    "    \"Model 2\": \"log(Helpfulness) = α + γ1*oValence + γ2*oArousal + β1*log(Length) + β2*Breadth + β3*Readability + β4*log(ReviewerPosts) + β5*log(Longevity) + β6~9*ReviewRating + β10*log(BizReviewCounts) + ε\",\n",
    "    \"Model 3\": \"log(Helpfulness) = α + γ1*oValence + γ2*oArousal + γ3*oArousal² + γ4*oArousal*oValence + β1*log(Length) + β2*Breadth + β3*Readability + β4*log(ReviewerPosts) + β5*log(Longevity) + β6~9*ReviewRating + β10*log(BizReviewCounts) + ε\",\n",
    "    \"Model 4\": \"log(Helpfulness) = α + γ1*oValence + γ2*oArousal + γ3*oArousal² + γ4*oArousal*oValence + γ5*oValence*oArousal² + β1*log(Length) + β2*Breadth + β3*Readability + β4*log(ReviewerPosts) + β5*log(Longevity) + β6~9*ReviewRating + β10*log(BizReviewCounts) + ε\"\n",
    "}\n",
    "\n",
    "print_model_summary(model1_result, \"Model 1 (參考模型)\", equations[\"Model 1\"])\n",
    "print_model_summary(model2_result, \"Model 2 (基礎模型)\", equations[\"Model 2\"])\n",
    "print_model_summary(model3_result, \"Model 3 (非線性模型)\", equations[\"Model 3\"])\n",
    "print_model_summary(model4_result, \"Model 4 (完整模型 - 方程式4)\", equations[\"Model 4\"])\n",
    "\n",
    "# 模型比較表\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"模型比較摘要\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "comparison_data = {\n",
    "    \"模型\": [\"Model 1 (參考)\", \"Model 2 (基礎)\", \"Model 3 (非線性)\", \"Model 4 (完整)\"],\n",
    "    \"觀測數\": [int(model1_result.nobs), int(model2_result.nobs), int(model3_result.nobs), int(model4_result.nobs)],\n",
    "    \"R²\": [f\"{model1_result.rsquared:.4f}\", f\"{model2_result.rsquared:.4f}\", f\"{model3_result.rsquared:.4f}\", f\"{model4_result.rsquared:.4f}\"],\n",
    "    \"Adj R²\": [f\"{model1_result.rsquared_adj:.4f}\", f\"{model2_result.rsquared_adj:.4f}\", f\"{model3_result.rsquared_adj:.4f}\", f\"{model4_result.rsquared_adj:.4f}\"],\n",
    "    \"AIC\": [f\"{model1_result.aic:.2f}\", f\"{model2_result.aic:.2f}\", f\"{model3_result.aic:.2f}\", f\"{model4_result.aic:.2f}\"],\n",
    "    \"BIC\": [f\"{model1_result.bic:.2f}\", f\"{model2_result.bic:.2f}\", f\"{model3_result.bic:.2f}\", f\"{model4_result.bic:.2f}\"],\n",
    "    \"F統計量\": [f\"{model1_result.fvalue:.2f}\", f\"{model2_result.fvalue:.2f}\", f\"{model3_result.fvalue:.2f}\", f\"{model4_result.fvalue:.2f}\"]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# =============================================\n",
    "# 第六部分：統計檢驗\n",
    "# =============================================\n",
    "\n",
    "# 進行嵌套模型F檢驗\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"嵌套模型F檢驗\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "def nested_f_test(restricted_model, full_model, model1_name, model2_name):\n",
    "    \"\"\"執行嵌套模型F檢驗\"\"\"\n",
    "    rss_restricted = restricted_model.ssr\n",
    "    rss_full = full_model.ssr\n",
    "    df_restricted = restricted_model.df_resid\n",
    "    df_full = full_model.df_resid\n",
    "    \n",
    "    # F統計量\n",
    "    f_stat = ((rss_restricted - rss_full) / (df_restricted - df_full)) / (rss_full / df_full)\n",
    "    p_value = 1 - stats.f.cdf(f_stat, df_restricted - df_full, df_full)\n",
    "    \n",
    "    print(f\"\\n{model1_name} vs {model2_name}:\")\n",
    "    print(f\"F統計量: {f_stat:.4f}\")\n",
    "    print(f\"p值: {p_value:.4e}\")\n",
    "    print(f\"自由度: ({df_restricted - df_full}, {df_full})\")\n",
    "    significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"不顯著\"\n",
    "    print(f\"結果: {significance}\")\n",
    "    \n",
    "    return f_stat, p_value\n",
    "\n",
    "# 執行F檢驗\n",
    "f1, p1 = nested_f_test(model1_result, model2_result, \"Model 1\", \"Model 2\")\n",
    "f2, p2 = nested_f_test(model2_result, model3_result, \"Model 2\", \"Model 3\")\n",
    "f3, p3 = nested_f_test(model3_result, model4_result, \"Model 3\", \"Model 4\")\n",
    "f4, p4 = nested_f_test(model1_result, model4_result, \"Model 1\", \"Model 4\")\n",
    "\n",
    "# =============================================\n",
    "# 第七部分：研究結論\n",
    "# =============================================\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"研究結論\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# 動態判斷最佳模型\n",
    "models_results = {\n",
    "    \"Model 1\": model1_result,\n",
    "    \"Model 2\": model2_result, \n",
    "    \"Model 3\": model3_result,\n",
    "    \"Model 4\": model4_result\n",
    "}\n",
    "\n",
    "# 找出最高Adjusted R²的模型\n",
    "best_r2_model = max(models_results.keys(), key=lambda x: models_results[x].rsquared_adj)\n",
    "best_r2_value = models_results[best_r2_model].rsquared_adj\n",
    "\n",
    "# 找出最低AIC的模型\n",
    "best_aic_model = min(models_results.keys(), key=lambda x: models_results[x].aic)\n",
    "best_aic_value = models_results[best_aic_model].aic\n",
    "\n",
    "print(\"1. 模型比較結果:\")\n",
    "print(f\"   - {best_r2_model}具有最高的解釋力 (Adj R² = {best_r2_value:.4f})\")\n",
    "print(f\"   - {best_aic_model}具有最低的AIC值 ({best_aic_value:.2f})，表示最佳的模型適配\")\n",
    "\n",
    "# 檢查AIC和Adj R²是否指向同一個模型\n",
    "if best_r2_model == best_aic_model:\n",
    "    print(f\"   - 基於Adj R²和AIC準則，{best_aic_model}表現最佳\")\n",
    "else:\n",
    "    print(f\"   - 注意：Adj R²和AIC準則指向不同模型，需進一步考慮研究目的選擇\")\n",
    "\n",
    "print(\"\\n2. 嵌套檢驗結果:\")\n",
    "if p1 < 0.05:\n",
    "    print(\"   - 加入線性情感效應顯著改善模型 (Model 2 > Model 1)\")\n",
    "else:\n",
    "    print(\"   - 加入線性情感效應未顯著改善模型\")\n",
    "\n",
    "if p2 < 0.05:\n",
    "    print(\"   - 加入非線性和交互效應顯著改善模型 (Model 3 > Model 2)\")\n",
    "else:\n",
    "    print(\"   - 加入非線性和交互效應未顯著改善模型\")\n",
    "\n",
    "if p3 < 0.05:\n",
    "    print(\"   - 加入oValence*oArousal²交互項顯著改善模型 (Model 4 > Model 3)\")\n",
    "else:\n",
    "    print(\"   - 加入oValence*oArousal²交互項未顯著改善模型\")\n",
    "\n",
    "print(\"\\n3. 關鍵發現:\")\n",
    "if 'oArousal_squared' in model4_result.params.index:\n",
    "    arousal_sq_coef = model4_result.params['oArousal_squared']\n",
    "    arousal_sq_pval = model4_result.pvalues['oArousal_squared']\n",
    "    if arousal_sq_pval < 0.05:\n",
    "        print(f\"   - 情感喚醒度存在顯著的非線性效應 (γ3 = {arousal_sq_coef:.4f}, p < 0.05)\")\n",
    "    else:   \n",
    "        print(\"   - 情感喚醒度的非線性效應不顯著\")\n",
    "\n",
    "if 'oArousal_oValence_interaction' in model4_result.params.index:\n",
    "    interaction_coef = model4_result.params['oArousal_oValence_interaction']\n",
    "    interaction_pval = model4_result.pvalues['oArousal_oValence_interaction']\n",
    "    if interaction_pval < 0.05:\n",
    "        print(f\"   - oArousal*oValence交互效應顯著 (γ4 = {interaction_coef:.4f}, p < 0.05)\")\n",
    "    else:\n",
    "        print(\"   - oArousal*oValence交互效應不顯著\")\n",
    "\n",
    "if 'oValence_oArousal_squared_interaction' in model4_result.params.index:\n",
    "    valence_arousal_sq_coef = model4_result.params['oValence_oArousal_squared_interaction']\n",
    "    valence_arousal_sq_pval = model4_result.pvalues['oValence_oArousal_squared_interaction']\n",
    "    if valence_arousal_sq_pval < 0.05:\n",
    "        print(f\"   - oValence*oArousal²交互效應顯著 (γ5 = {valence_arousal_sq_coef:.4f}, p < 0.05)\")\n",
    "        print(\"   - 這表示情感效價會調節情緒喚醒度的倒U型關係\")\n",
    "    else:\n",
    "        print(\"   - oValence*oArousal²交互效應不顯著\")\n",
    "\n",
    "print(f\"\\n4. 方程式(4)的理論意義:\")\n",
    "print(f\"   - γ5項 (oValence*oArousal²) 代表情感效價對情緒喚醒度倒U型關係的調節作用\")\n",
    "print(f\"   - 正的γ5係數表示：正面情感會加強高喚醒度對有用性的負面影響\")\n",
    "print(f\"   - 負的γ5係數表示：正面情感會緩解高喚醒度對有用性的負面影響\")\n",
    "\n",
    "print(f\"\\n5. 最終建議:\")\n",
    "if best_r2_model == best_aic_model:\n",
    "    print(f\"   基於Adj R²和AIC準則的一致性，推薦使用{best_aic_model}作為最終分析模型。\")\n",
    "else:\n",
    "    print(f\"   Adj R²支持{best_r2_model}，AIC支持{best_aic_model}。\")\n",
    "    print(f\"   建議根據研究目的選擇：\")\n",
    "    print(f\"   - 若重視解釋力最大化：選擇{best_r2_model}\")\n",
    "    print(f\"   - 若重視模型簡約性：選擇{best_aic_model}\")\n",
    "    print(f\"   - 若理論假設重要：評估F檢驗結果決定是否需要複雜模型\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"分析完成！包含方程式(4)的完整交互項分析\")\n",
    "print(f\"{'='*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "轉折點移動分析(for H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def analyze_turning_point_sensitivity(model_result, data_df):\n",
    "    \"\"\"\n",
    "    分析轉折點隨情感效價變化的移動程度\n",
    "    \n",
    "    轉折點公式: oArousal* = (-γ2 - γ4×oValence) / (2×γ3)\n",
    "    對oValence的一階導數: d(oArousal*)/d(oValence) = -γ4 / (2×γ3)\n",
    "    \n",
    "    參數:\n",
    "    - model_result: 完整模型(Model 3)的回歸結果\n",
    "    - data_df: 包含原始數據的DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"轉折點移動程度分析 - 情感效價敏感性\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # 提取回歸係數\n",
    "    gamma2 = model_result.params['oArousal']  # oArousal線性項\n",
    "    gamma3 = model_result.params['oArousal_squared']  # oArousal平方項  \n",
    "    gamma4 = model_result.params['oArousal_oValence_interaction']  # 交互項\n",
    "    \n",
    "    # 提取p值\n",
    "    p_gamma2 = model_result.pvalues['oArousal']\n",
    "    p_gamma3 = model_result.pvalues['oArousal_squared']\n",
    "    p_gamma4 = model_result.pvalues['oArousal_oValence_interaction']\n",
    "    \n",
    "    print(f\"模型係數:\")\n",
    "    print(f\"γ2 (oArousal): {gamma2:.6f} (p = {p_gamma2:.6f})\")\n",
    "    print(f\"γ3 (oArousal²): {gamma3:.6f} (p = {p_gamma3:.6f})\")\n",
    "    print(f\"γ4 (oArousal×oValence): {gamma4:.6f} (p = {p_gamma4:.6f})\")\n",
    "    \n",
    "    # 計算轉折點對效價的敏感性\n",
    "    print(f\"\\n轉折點公式: oArousal* = (-γ2 - γ4×oValence) / (2×γ3)\")\n",
    "    print(f\"轉折點公式: oArousal* = (-{gamma2:.6f} - {gamma4:.6f}×oValence) / (2×{gamma3:.6f})\")\n",
    "    \n",
    "    # 一階導數計算\n",
    "    sensitivity = -gamma4 / (2 * gamma3)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"轉折點敏感性分析\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"一階導數公式: d(oArousal*)/d(oValence) = -γ4 / (2×γ3)\")\n",
    "    print(f\"一階導數計算: d(oArousal*)/d(oValence) = -{gamma4:.6f} / (2×{gamma3:.6f})\")\n",
    "    print(f\"敏感性係數: {sensitivity:.6f}\")\n",
    "    \n",
    "    # 敏感性的標準誤計算 (Delta方法)\n",
    "    # 對於 f(γ3,γ4) = -γ4 / (2*γ3)\n",
    "    # 偏導數:\n",
    "    df_dgamma3 = gamma4 / (2 * gamma3**2)\n",
    "    df_dgamma4 = -1 / (2 * gamma3)\n",
    "    \n",
    "    # 取得方差和協方差\n",
    "    var_gamma3 = model_result.cov_params().loc['oArousal_squared', 'oArousal_squared']\n",
    "    var_gamma4 = model_result.cov_params().loc['oArousal_oValence_interaction', 'oArousal_oValence_interaction']\n",
    "    cov_gamma3_gamma4 = model_result.cov_params().loc['oArousal_squared', 'oArousal_oValence_interaction']\n",
    "    \n",
    "    # 計算敏感性的方差\n",
    "    sensitivity_var = (df_dgamma3**2 * var_gamma3 + \n",
    "                      df_dgamma4**2 * var_gamma4 + \n",
    "                      2 * df_dgamma3 * df_dgamma4 * cov_gamma3_gamma4)\n",
    "    \n",
    "    sensitivity_se = np.sqrt(sensitivity_var)\n",
    "    sensitivity_t_stat = sensitivity / sensitivity_se\n",
    "    sensitivity_p_value = 2 * (1 - stats.t.cdf(abs(sensitivity_t_stat), model_result.df_resid))\n",
    "    \n",
    "    # 敏感性顯著性\n",
    "    if sensitivity_p_value < 0.001:\n",
    "        significance = \"***\"\n",
    "    elif sensitivity_p_value < 0.01:\n",
    "        significance = \"**\"\n",
    "    elif sensitivity_p_value < 0.05:\n",
    "        significance = \"*\"\n",
    "    else:\n",
    "        significance = \"\"\n",
    "    \n",
    "    print(f\"\\n敏感性統計檢驗:\")\n",
    "    print(f\"標準誤: {sensitivity_se:.6f}\")\n",
    "    print(f\"t統計量: {sensitivity_t_stat:.3f}\")\n",
    "    print(f\"p值: {sensitivity_p_value:.6f} {significance}\")\n",
    "    \n",
    "    # 敏感性解釋\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"敏感性解釋\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if sensitivity_p_value < 0.05:\n",
    "        print(f\"✓ 轉折點敏感性顯著 (p < 0.05)\")\n",
    "        \n",
    "        if sensitivity > 0:\n",
    "            direction = \"正向移動\"\n",
    "            explanation = \"情感效價增加時，轉折點向更高的喚醒度移動\"\n",
    "        else:\n",
    "            direction = \"負向移動\"\n",
    "            explanation = \"情感效價增加時，轉折點向更低的喚醒度移動\"\n",
    "            \n",
    "        print(f\"移動方向: {direction}\")\n",
    "        print(f\"解釋: {explanation}\")\n",
    "        print(f\"移動幅度: 情感效價每增加1個單位，轉折點移動 {abs(sensitivity):.6f} 個喚醒度單位\")\n",
    "    else:\n",
    "        print(f\"✗ 轉折點敏感性不顯著 (p = {sensitivity_p_value:.6f})\")\n",
    "        print(\"轉折點不隨情感效價顯著變化\")\n",
    "    \n",
    "    # 數據範圍分析\n",
    "    arousal_data = data_df['oArousal']\n",
    "    valence_data = data_df['oValence']\n",
    "    \n",
    "    arousal_range = arousal_data.max() - arousal_data.min()\n",
    "    valence_range = valence_data.max() - valence_data.min()\n",
    "    \n",
    "    print(f\"\\n數據範圍:\")\n",
    "    print(f\"oArousal範圍: [{arousal_data.min():.4f}, {arousal_data.max():.4f}] (寬度: {arousal_range:.4f})\")\n",
    "    print(f\"oValence範圍: [{valence_data.min():.4f}, {valence_data.max():.4f}] (寬度: {valence_range:.4f})\")\n",
    "    \n",
    "    # 實際移動幅度計算\n",
    "    if sensitivity_p_value < 0.05:\n",
    "        max_movement = abs(sensitivity) * valence_range\n",
    "        relative_movement = max_movement / arousal_range * 100\n",
    "        \n",
    "        print(f\"\\n實際移動分析:\")\n",
    "        print(f\"在整個效價範圍內，轉折點最大移動距離: {max_movement:.4f} 個喚醒度單位\")\n",
    "        print(f\"相對於喚醒度總範圍的移動比例: {relative_movement:.2f}%\")\n",
    "        \n",
    "        if relative_movement > 50:\n",
    "            movement_level = \"大幅移動\"\n",
    "        elif relative_movement > 20:\n",
    "            movement_level = \"中等移動\"  \n",
    "        elif relative_movement > 10:\n",
    "            movement_level = \"小幅移動\"\n",
    "        else:\n",
    "            movement_level = \"微小移動\"\n",
    "            \n",
    "        print(f\"移動程度評估: {movement_level}\")\n",
    "    \n",
    "    # 不同效價水平下的具體轉折點計算\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"不同效價水平下的轉折點位置\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 定義效價水平\n",
    "    valence_levels = [\n",
    "        (\"低效價 (10%)\", np.percentile(valence_data, 10)),\n",
    "        (\"中低效價 (25%)\", np.percentile(valence_data, 25)),\n",
    "        (\"中效價 (50%)\", np.percentile(valence_data, 50)),\n",
    "        (\"中高效價 (75%)\", np.percentile(valence_data, 75)),\n",
    "        (\"高效價 (90%)\", np.percentile(valence_data, 90))\n",
    "    ]\n",
    "    \n",
    "    print(f\"{'效價水平':<15} {'效價值':<10} {'轉折點':<12} {'相對變化':<12}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    baseline_turning_point = None\n",
    "    \n",
    "    for i, (val_name, val_level) in enumerate(valence_levels):\n",
    "        # 計算轉折點\n",
    "        turning_point = (-gamma2 - gamma4 * val_level) / (2 * gamma3)\n",
    "        \n",
    "        if i == 0:  # 設定基準點\n",
    "            baseline_turning_point = turning_point\n",
    "            relative_change = 0.0\n",
    "        else:\n",
    "            relative_change = turning_point - baseline_turning_point\n",
    "            \n",
    "        print(f\"{val_name:<15} {val_level:<10.4f} {turning_point:<12.4f} {relative_change:<+12.4f}\")\n",
    "    \n",
    "    # 總結\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"總結\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if sensitivity_p_value < 0.05:\n",
    "        print(f\"✓ 轉折點顯著隨情感效價變化\")\n",
    "        print(f\"  - 敏感性係數: {sensitivity:.6f} {significance}\")\n",
    "        print(f\"  - 移動方向: {'正向' if sensitivity > 0 else '負向'}\")\n",
    "        if sensitivity_p_value < 0.05:\n",
    "            print(f\"  - 在效價範圍內最大移動: {max_movement:.4f} 個單位 ({relative_movement:.2f}%)\")\n",
    "            print(f\"  - 移動程度: {movement_level}\")\n",
    "    else:\n",
    "        print(f\"✗ 轉折點不隨情感效價顯著變化\")\n",
    "        print(f\"  - 敏感性係數不顯著 (p = {sensitivity_p_value:.6f})\")\n",
    "    \n",
    "    return {\n",
    "        'sensitivity': sensitivity,\n",
    "        'sensitivity_se': sensitivity_se,\n",
    "        'sensitivity_t_stat': sensitivity_t_stat,\n",
    "        'sensitivity_p_value': sensitivity_p_value,\n",
    "        'significant': sensitivity_p_value < 0.05,\n",
    "        'max_movement': max_movement if sensitivity_p_value < 0.05 else 0,\n",
    "        'relative_movement_percent': relative_movement if sensitivity_p_value < 0.05 else 0,\n",
    "        'turning_points_by_valence': valence_levels\n",
    "    }\n",
    "\n",
    "# 使用範例\n",
    "# sensitivity_results = analyze_turning_point_sensitivity(model3_result, df_model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# 讀取原始 CSV 檔案\n",
    "df = pd.read_csv('amazon_2023_with_breadth_readability.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# 讀取原始 CSV 檔案\n",
    "df = pd.read_csv('amazon_2023_with_breadth_readability.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
